{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Contains all functions used in this thesis.\n",
    "\n",
    "### 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import math\n",
    "import warnings\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from io import StringIO\n",
    "import statsmodels.api as sm\n",
    "from spellchecker import SpellChecker\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_search(startdate, enddate, topic, account, save):\n",
    "        \n",
    "        \"\"\"\n",
    "        Scrapes tweets of a profile during a given time frame with the .Profile-function of the package TWINT.\n",
    "        INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "                enddate:          STRING, end date in yyyy-mm-dd format\n",
    "                topic:            STRING, name of topic, used to name folders\n",
    "                main_account:     STRING, name of main account\n",
    "        \"\"\"\n",
    "        \n",
    "        # scrape using TWINT-package\n",
    "        c = twint.Config()\n",
    "        c.Until = enddate\n",
    "        c.Since = startdate\n",
    "        c.Username = account\n",
    "        c.Retweets = \"True\"\n",
    "        c.Hide_output = True\n",
    "        c.Output = save + account + '_' + startdate + '_' + enddate + '.txt'\n",
    "        twint.run.Profile(c)\n",
    "        \n",
    "        # if the scraping was successful, prepare data\n",
    "        if os.path.exists(save + account + '_' + startdate + '_' + enddate + '.txt'):            \n",
    "            data = pd.read_fwf(save + account + '_' + startdate + '_' + enddate + '.txt',header=None,encoding=\"UTF-8\")\n",
    "            if len(data.columns)>3:\n",
    "                # prepare data frame for all scraped data\n",
    "                data = data.dropna(subset=[0,1,2,3,4]).reset_index(drop=True)\n",
    "                data_clean = pd.DataFrame(columns=['id', 'date', 'time', 'acc', 'tweet'])\n",
    "                for i in range(len(data)):\n",
    "                    try:\n",
    "                        x = list(data.loc[i])\n",
    "                        s = ''\n",
    "                        for j in range(len(x)):\n",
    "                            s = s + str(x[j]) + ' '\n",
    "                        s = s.rstrip(' ')\n",
    "                        data_clean.loc[i,'id'] = s.split(' ')[0]\n",
    "                        data_clean.loc[i,'date'] = s.split(' ')[1]\n",
    "                        data_clean.loc[i,'time'] = s.split(' ')[2]\n",
    "                        data_clean.loc[i,'acc'] = s.split('<')[1].split('>')[0]\n",
    "                        data_clean.loc[i,'tweet'] = s.split('<')[1].split('>')[1][1:].replace(' nan', '')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                data_clean = data_clean.dropna().reset_index(drop=True)\n",
    "\n",
    "                # prepare data frame for retweets\n",
    "                retweets = data_clean.loc[data_clean['tweet'].str.contains('RT @')].reset_index(drop=True)\n",
    "                for i in range(len(retweets)):\n",
    "                    retweets.loc[i,'rt_acc'] = retweets.loc[i,'tweet'].split(' ')[1].rstrip(':')\n",
    "                \n",
    "                # prepare data frame for non-retweets\n",
    "                noretweets = data_clean.loc[~data_clean['tweet'].str.contains('RT @')].reset_index(drop=True)\n",
    "                print('Profile searched @' + account + ' - tweets/replies found: ' + str(len(noretweets)) + ' (' + str(len(data_clean)) + ' total)')\n",
    "                \n",
    "                # save\n",
    "                data_clean.to_csv(save + account + '_' + startdate + '_' + enddate + '.csv',encoding='utf-8',index=None)\n",
    "                retweets.to_csv(save + 'retweets_' + account + '.csv',encoding='utf-8',index=None)\n",
    "                noretweets.to_csv(save + 'select_' + account + '.csv',encoding='utf-8',index=None)\n",
    "            else:\n",
    "                print('Issue when reading tweets. Probably mentioned many accounts. Account @' + account + ' skipped.')\n",
    "        else:\n",
    "            print('No tweets scraped. Select other account or timeframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_search(startdate, enddate, topic, account, save, search):\n",
    "        \n",
    "        \"\"\"\n",
    "        Scrapes tweets of a profile during a given time frame with the .Search-function of the package TWINT.\n",
    "        INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "                enddate:          STRING, end date in yyyy-mm-dd format\n",
    "                topic:            STRING, name of topic, used to name folders\n",
    "                account:          STRING, name of account\n",
    "                save:             STRING, path of directory\n",
    "                search:           STRING, keywords of search, split by 'OR'\n",
    "        \"\"\"\n",
    "        \n",
    "        # scrape using TWINT-package\n",
    "        c = twint.Config()\n",
    "        c.Until = enddate\n",
    "        c.Since = startdate\n",
    "        c.Username = account\n",
    "        c.Search = search\n",
    "        c.Hide_output = True\n",
    "        c.Output = save + account + '_' + startdate + '_' + enddate + '.txt'\n",
    "        twint.run.Search(c)\n",
    "        \n",
    "        # if the scraping was successful, prepare data\n",
    "        if os.path.exists(save + account + '_' + startdate + '_' + enddate + '.txt'):\n",
    "            data = pd.read_fwf(save + account + '_' + startdate + '_' + enddate + '.txt',header=None,encoding=\"UTF-8\")\n",
    "            if len(data.columns)>3:\n",
    "                data = data.dropna(subset=[0,1,2,3,4]).reset_index(drop=True)\n",
    "                data_clean = pd.DataFrame(columns=['id', 'date', 'time', 'acc', 'tweet'])\n",
    "                for i in range(len(data)):\n",
    "                    try:\n",
    "                        x = list(data.loc[i])\n",
    "                        s = ''\n",
    "                        for j in range(len(x)):\n",
    "                            s = s + str(x[j]) + ' '\n",
    "                        s = s.rstrip(' ')\n",
    "                        data_clean.loc[i,'id'] = s.split(' ')[0]\n",
    "                        data_clean.loc[i,'date'] = s.split(' ')[1]\n",
    "                        data_clean.loc[i,'time'] = s.split(' ')[2]\n",
    "                        data_clean.loc[i,'acc'] = s.split('<')[1].split('>')[0]\n",
    "                        data_clean.loc[i,'tweet'] = s.split('<')[1].split('>')[1][1:].replace(' nan', '')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                data_clean = data_clean.dropna().reset_index(drop=True)\n",
    "                data_clean.to_csv(save + 'select_' + account + '.csv',encoding='utf-8',index=None)\n",
    "                print('Profile searched @' + account + ' - tweets/replies found: ' + str(len(data_clean)))\n",
    "            else:\n",
    "                print('Issue when reading tweets. Probably mentioned many accounts. Account @' + account + ' skipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions_search(startdate, enddate, topic, account, save):\n",
    "        \n",
    "        \"\"\"\n",
    "        Searches all mentions of a profile.\n",
    "        INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "                enddate:          STRING, end date in yyyy-mm-dd format\n",
    "                topic:            STRING, name of topic, used to name folders\n",
    "                account:          STRING, name of account\n",
    "                save:             STRING, path of directory\n",
    "        \"\"\"\n",
    "        \n",
    "        # scrape using TWINT-package\n",
    "        c = twint.Config()\n",
    "        c.Until = enddate\n",
    "        c.Since = startdate\n",
    "        c.Search = '@' + account\n",
    "        c.Hide_output = True\n",
    "        c.Output = save + account + '_' + startdate + '_' + enddate + '.txt'\n",
    "        twint.run.Search(c)\n",
    "        \n",
    "        # if the scraping was successful, prepare data\n",
    "        if os.path.exists(save + account + '_' + startdate + '_' + enddate + '.txt'):\n",
    "            data = pd.read_fwf(save + account + '_' + startdate + '_' + enddate + '.txt',header=None,encoding=\"UTF-8\")\n",
    "            if len(data.columns)>3:\n",
    "                data = data.dropna(subset=[0,1,2,3,4]).reset_index(drop=True)\n",
    "                data_clean = pd.DataFrame(columns=['id', 'date', 'time', 'acc', 'tweet'])\n",
    "                for i in range(len(data)):\n",
    "                    try:\n",
    "                        x = list(data.loc[i])\n",
    "                        s = ''\n",
    "                        for j in range(len(x)):\n",
    "                            s = s + str(x[j]) + ' '\n",
    "                        s = s.rstrip(' ')\n",
    "                        data_clean.loc[i,'id'] = s.split(' ')[0]\n",
    "                        data_clean.loc[i,'date'] = s.split(' ')[1]\n",
    "                        data_clean.loc[i,'time'] = s.split(' ')[2]\n",
    "                        data_clean.loc[i,'acc'] = s.split('<')[1].split('>')[0]\n",
    "                        data_clean.loc[i,'tweet'] = s.split('<')[1].split('>')[1][1:].replace(' nan', '')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                data_clean = data_clean.dropna().reset_index(drop=True)\n",
    "                data_clean.to_csv(save + 'mentions_' + account + startdate + '_' + enddate + '.csv',encoding='utf-8',index=None)\n",
    "                print('Profile searched @' + account + ' - mentions found: ' + str(len(data_clean)))\n",
    "            else:\n",
    "                print('Issue when reading tweets. Probably mentioned many accounts. Account @' + account + ' skipped.')                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_search_retweets(startdate, enddate, topic, account, save_rt, save_sel, rtaccs, search):\n",
    "        \n",
    "        \"\"\"\n",
    "        Checks if an account retweeted an account from a list of accounts. If yes, it is searched for a list of hashtags.\n",
    "        INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "                enddate:          STRING, end date in yyyy-mm-dd format\n",
    "                topic:            STRING, name of topic, used to name folders\n",
    "                account:          STRING, name of account\n",
    "                save_rt:          STRING, path of directory for retweets-part\n",
    "                save_sel:         STRING, path of directory for part searching for hashtag usage\n",
    "                rtaccs:           LIST, list of accounts\n",
    "                search:           STRING, hashtags, split by 'OR'\n",
    "        \"\"\"\n",
    "        \n",
    "        # scrape using TWINT-package\n",
    "        c = twint.Config()\n",
    "        c.Until = enddate\n",
    "        c.Since = startdate\n",
    "        c.Username = account\n",
    "        c.Retweets = \"True\"\n",
    "        c.Hide_output = True\n",
    "        c.Output = save_rt + account + '_' + startdate + '_' + enddate + '.txt'\n",
    "        twint.run.Profile(c)\n",
    "\n",
    "        # if the scraping was successful, prepare data\n",
    "        if os.path.exists(save_rt + account + '_' + startdate + '_' + enddate + '.txt'):\n",
    "            data = pd.read_fwf(save_rt + account + '_' + startdate + '_' + enddate + '.txt',header=None,encoding=\"utf-8\")\n",
    "            if len(data.columns)>3:\n",
    "                # prepare all data\n",
    "                data = data.dropna(subset=[0,1,2,3,4]).reset_index(drop=True)\n",
    "                data_clean = pd.DataFrame(columns=['id', 'date', 'time', 'acc', 'tweet'])\n",
    "                for i in range(len(data)):\n",
    "                    try:\n",
    "                        x = list(data.loc[i])\n",
    "                        s = ''\n",
    "                        for j in range(len(x)):\n",
    "                            s = s + str(x[j]) + ' '\n",
    "                        s = s.rstrip(' ')\n",
    "                        data_clean.loc[i,'id'] = s.split(' ')[0]\n",
    "                        data_clean.loc[i,'date'] = s.split(' ')[1]\n",
    "                        data_clean.loc[i,'time'] = s.split(' ')[2]\n",
    "                        data_clean.loc[i,'acc'] = s.split('<')[1].split('>')[0]\n",
    "                        data_clean.loc[i,'tweet'] = s.split('<')[1].split('>')[1][1:].replace(' nan', '')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                data_clean = data_clean.dropna().reset_index(drop=True)\n",
    "                data_clean.to_csv(save_rt + account + '_' + startdate + '_' + enddate + '.csv',encoding='utf-8',index=None)\n",
    "                \n",
    "                # reduce to retweets, call search_search if account retweeted an account in the list\n",
    "                retweets = data_clean.loc[data_clean['tweet'].str.contains('RT @')].reset_index(drop=True)\n",
    "                if len(retweets)>0:\n",
    "                    for i in range(len(retweets)):\n",
    "                        retweets.loc[i,'rt_acc'] = retweets.loc[i,'tweet'].split(' ')[1].rstrip(':')\n",
    "                    rtcounts = retweets['rt_acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "                    rt_select = rtcounts.loc[rtcounts['acc'].isin(rtaccs)].reset_index(drop=True)\n",
    "                    if len(rt_select)>0:\n",
    "                        search_search(startdate, enddate, topic, account, save_sel, search)\n",
    "            else:\n",
    "                print('Issue when reading tweets. Probably mentioned many accounts. Account @' + account + ' skipped.')  \n",
    "        else:\n",
    "            # search most recent tweets as time frame may not be reachable\n",
    "            print('@' + account + ' has too much tweets or is private/locked. Timeframe not directly reachable. Trying to find retweets without timeframe.')\n",
    "            profile_search_retweets_current(startdate, enddate, topic, account, save_rt, save_sel, rtaccs, search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_search_retweets_current(startdate, enddate, topic, account, save_rt, save_sel, rtaccs, search):\n",
    "        \n",
    "        \"\"\"\n",
    "        Checks if an account retweeted an account from a list of accounts. If yes, it is searched for a list of hashtags.\n",
    "        This function uses the most recent tweets in case the time frame given may not be directly reachable.\n",
    "        INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "                enddate:          STRING, end date in yyyy-mm-dd format\n",
    "                topic:            STRING, name of topic, used to name folders\n",
    "                account:          STRING, name of account\n",
    "                save_rt:          STRING, path of directory for retweets-part\n",
    "                save_sel:         STRING, path of directory for part searching for hashtag usage\n",
    "                rtaccs:           LIST, list of accounts\n",
    "                search:           STRING, hashtags, split by 'OR'\n",
    "        \"\"\"\n",
    "        \n",
    "        # scrape using TWINT-package        \n",
    "        c = twint.Config()\n",
    "        c.Username = account\n",
    "        c.Retweets = \"True\"\n",
    "        c.Hide_output = True\n",
    "        c.Output = save_rt + account + '_current.txt'\n",
    "        twint.run.Profile(c)\n",
    "\n",
    "        # if the scraping was successful, prepare data\n",
    "        if os.path.exists(save_rt + account + '_current.txt'):\n",
    "            data = pd.read_fwf(save_rt + account + '_current.txt',header=None,encoding='utf-8')\n",
    "            if len(data.columns)>3:\n",
    "                # prepare all data\n",
    "                data = data.dropna(subset=[0,1,2,3,4]).reset_index(drop=True)\n",
    "                data_clean = pd.DataFrame(columns=['id', 'date', 'time', 'acc', 'tweet'])\n",
    "                for i in range(len(data)):\n",
    "                    try:\n",
    "                        x = list(data.loc[i])\n",
    "                        s = ''\n",
    "                        for j in range(len(x)):\n",
    "                            s = s + str(x[j]) + ' '\n",
    "                        s = s.rstrip(' ')\n",
    "                        data_clean.loc[i,'id'] = s.split(' ')[0]\n",
    "                        data_clean.loc[i,'date'] = s.split(' ')[1]\n",
    "                        data_clean.loc[i,'time'] = s.split(' ')[2]\n",
    "                        data_clean.loc[i,'acc'] = s.split('<')[1].split('>')[0]\n",
    "                        data_clean.loc[i,'tweet'] = s.split('<')[1].split('>')[1][1:].replace(' nan', '')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                data_clean = data_clean.dropna().reset_index(drop=True)\n",
    "                data_clean.to_csv(save_rt + account + '_current.csv',encoding='utf-8',index=None)\n",
    "\n",
    "                # reduce to retweets, call search_search if account retweeted an account in the list\n",
    "                retweets = data_clean.loc[data_clean['tweet'].str.contains('RT @')].reset_index(drop=True)\n",
    "                if len(retweets)>0:\n",
    "                    for i in range(len(retweets)):\n",
    "                        retweets.loc[i,'rt_acc'] = retweets.loc[i,'tweet'].split(' ')[1].rstrip(':')\n",
    "                    rtcounts = retweets['rt_acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "                    rt_select = rtcounts.loc[rtcounts['acc'].isin(rtaccs)].reset_index(drop=True)\n",
    "                    if len(rt_select)>0:\n",
    "                        search_search(startdate, enddate, topic, account, save_sel, search)\n",
    "            else:\n",
    "                print('Issue when reading tweets. Probably mentioned many accounts. Account @' + account + ' skipped.')\n",
    "        else:\n",
    "            print('No tweets scraped. Account is probably private or locked.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_acc(startdate, enddate, topic, account, directory):   \n",
    "\n",
    "    \"\"\"\n",
    "    Scrapes the tweets from a given main account.\n",
    "    INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "            enddate:          STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            account:          STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "    \n",
    "    # set directories\n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    retweets_dir = topic_dir + '02_retweets/'\n",
    "    \n",
    "    # generate folder structure if it does not exist yet\n",
    "    if not os.path.exists(topic_dir): \n",
    "        os.makedirs(topic_dir)\n",
    "        os.makedirs(main_dir)\n",
    "        os.makedirs(retweets_dir)\n",
    "    \n",
    "    # scrape tweets of main account\n",
    "    profile_search(startdate, enddate, topic, account, main_dir)\n",
    "    \n",
    "    # generate list of hashtags used by main account\n",
    "    select = pd.read_csv(main_dir + 'select_' + account + '.csv',encoding='UTF-8')\n",
    "    select = select.loc[select['tweet'].str.contains('#')].reset_index(drop=True)\n",
    "    hashtags = []\n",
    "    for i in range(len(select)):\n",
    "        split = select.loc[i,'tweet'].split('#')\n",
    "        for j in range(1,len(split)):\n",
    "            if split[j].split(' ')[0].lower().translate(str.maketrans('', '', string.punctuation)) != '':\n",
    "                hashtags.append(split[j].split(' ')[0].lower().translate(str.maketrans('', '', string.punctuation)))    \n",
    "    # too much hashtags crashes TWINT --> reduce to max 20 (most used)\n",
    "    if len(hashtags)<=20:\n",
    "        hashtags = list(dict.fromkeys(hashtags))\n",
    "        pd.DataFrame(hashtags).to_csv(main_dir + 'hashtag.csv',encoding='utf-8',index=None,header=None)\n",
    "    else:\n",
    "        hashtags_df = pd.DataFrame()\n",
    "        hashtags_df['hashtags'] = hashtags\n",
    "        counts = hashtags_df['hashtags'].value_counts().rename_axis('hashtag').reset_index(name='count')\n",
    "        sel_hashtags = counts[0:20]\n",
    "        pd.DataFrame(sel_hashtags['hashtag']).to_csv(main_dir + 'hashtag.csv',encoding='utf-8',index=None,header=None)\n",
    "    \n",
    "    # generate list of scraped accounts\n",
    "    scraped_accs = pd.DataFrame(columns=['acc'])\n",
    "    scraped_accs.loc[0,'acc'] = account\n",
    "    scraped_accs.to_csv(topic_dir + 'scraped_accs.csv',encoding='utf-8',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hstring(directory, topic):\n",
    "\n",
    "    \"\"\"\n",
    "    Converting the hashtag.csv to a string, which is usable by TWINT.\n",
    "    INPUTS: directory:        STRING, directory\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "    OUTPUT: hstring:          STRING, hashtags, split by 'OR'\n",
    "    \"\"\"    \n",
    "    \n",
    "    # set directories\n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    \n",
    "    # import hashtags.csv\n",
    "    hashtags = pd.read_csv(main_dir + 'hashtag.csv', header=None)\n",
    "    \n",
    "    # convert to a string, split by 'OR'\n",
    "    hlist = list(hashtags[0])\n",
    "    hstring = ''\n",
    "    for i in range(len(hlist)):\n",
    "        if i==len(hlist)-1:\n",
    "            hstring = hstring + hlist[i]\n",
    "        else:\n",
    "            hstring = hstring + hlist[i] + ' OR '\n",
    "    return hstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_retweets(startdate, enddate, topic, mainaccount, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the tweets of accounts retweeted by the main account.\n",
    "    INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "            enddate:          STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            mainaccount:      STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "    \n",
    "    # set directories    \n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    retweets_dir = topic_dir + '02_retweets/'\n",
    "    \n",
    "    # import dataset of main account's retweets\n",
    "    retweets = pd.read_csv(main_dir + 'retweets_' + mainaccount + '.csv')\n",
    "    \n",
    "    if len(retweets)>0:\n",
    "        \n",
    "        # create list of retweeted accounts\n",
    "        rtcounts = retweets['rt_acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "        print('Retweeted accounts found: ' + str(len(rtcounts)))\n",
    "        rt_accs = list(rtcounts['acc'])\n",
    "        \n",
    "        # create list of hashtags in a string\n",
    "        hstring = create_hstring(directory, topic)\n",
    "        \n",
    "        # create list of accounts already scraped\n",
    "        scraped_accs = pd.read_csv(topic_dir + 'scraped_accs.csv',encoding='utf-8')\n",
    "        scraped_list = list(scraped_accs['acc'])\n",
    "        \n",
    "        # for each retweeted account, scrape tweets containing one of the selected hashtags\n",
    "        for i in range(len(rt_accs)):\n",
    "            try:\n",
    "                account = rt_accs[i].lstrip('@')\n",
    "                if account not in scraped_list:\n",
    "                    search_search(startdate, enddate, topic, account, retweets_dir, hstring)\n",
    "                    scraped_list.append(account)\n",
    "            except ValueError:\n",
    "                print('Account not found: @' + account)\n",
    "                continue\n",
    "        \n",
    "        # update file listing scraped accounts\n",
    "        scraped_accs = pd.DataFrame(columns=['acc'])\n",
    "        scraped_accs['acc'] = scraped_list\n",
    "        scraped_accs.to_csv(topic_dir + 'scraped_accs.csv',encoding='utf-8',index=None)\n",
    "        \n",
    "    else:\n",
    "        print('No retweeted accounts found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_1_2(topic, mainacc, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Combines the results of stages 1 and 2 of the bubble-generating procedure.\n",
    "    INPUTS: topic:            STRING, name of topic, used to name folders\n",
    "            mainaccount:      STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "\n",
    "    # set directories \n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    retweets_dir = topic_dir + '02_retweets/'\n",
    "    \n",
    "    # add tweets of stage 1\n",
    "    comb = pd.DataFrame(columns=['id','date','time','acc','tweet'])\n",
    "    comb = comb.append(pd.read_csv(main_dir + 'select_' + mainacc + '.csv', encoding='utf-8'))\n",
    "    \n",
    "    # add tweets of stage 2\n",
    "    for index, filename in enumerate(os.listdir(retweets_dir)):\n",
    "        if filename.endswith('.csv') and filename.startswith('select_'):\n",
    "            comb=comb.append(pd.read_csv(retweets_dir + filename, encoding='utf-8'))\n",
    "    \n",
    "    # save combined dataframe\n",
    "    comb.to_csv(topic_dir + 'combine_1_2.csv',encoding='utf-8',index=None)\n",
    "    print('Tweets found (step 1&2): ' + str(len(comb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mentions(startdate, enddate, topic, mainaccount, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects tweets mentioning the main account.\n",
    "    INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "            enddate:          STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            mainaccount:      STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "    \n",
    "    # set directories        \n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    retweets_dir = topic_dir + '02_retweets/'\n",
    "    mentions_dir = topic_dir + '03_mentions/'\n",
    "    mentions_rt_dir = mentions_dir + 'retweets/'\n",
    "    mentions_sel_dir = mentions_dir + 'select/'\n",
    "    \n",
    "    # create non-existing folders\n",
    "    if not os.path.exists(mentions_dir): \n",
    "        os.makedirs(mentions_dir)\n",
    "        os.makedirs(mentions_rt_dir)\n",
    "        os.makedirs(mentions_sel_dir)\n",
    "    \n",
    "    # scrape the tweets\n",
    "    mentions_search(startdate, enddate, topic, mainaccount, mentions_dir)\n",
    "    \n",
    "    # import results from scraping\n",
    "    mentions = pd.read_csv(mentions_dir + 'mentions_' + mainaccount + startdate + '_' + enddate + '.csv',encoding='utf-8')\n",
    "    \n",
    "    # generating list of accounts mentioning main account more than 5 times\n",
    "    mentioncounts = mentions['acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "    mentions_over_5 = mentioncounts.loc[mentioncounts['count']>5]\n",
    "    print('mentions_over_5: ' + str(len(mentions_over_5)))\n",
    "    accounts = list(mentions_over_5['acc'])\n",
    "    \n",
    "    # import accounts retweeted by main account\n",
    "    retweets = pd.read_csv(main_dir + 'retweets_' + mainaccount + '.csv')\n",
    "    \n",
    "    # generate list of main account and the retweeted accounts\n",
    "    if len(retweets)>0:\n",
    "        rtcounts = retweets['rt_acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "        acclist = list(rtcounts['acc'])\n",
    "        acclist.append('@' + mainaccount)\n",
    "    else:\n",
    "        acclist = ['@' + mainaccount]\n",
    "    \n",
    "    # create list of hashtags in a string\n",
    "    hstring = create_hstring(directory, topic)\n",
    "\n",
    "    # create list of accounts already scraped\n",
    "    scraped_accs = pd.read_csv(topic_dir + 'scraped_accs.csv',encoding='utf-8')\n",
    "    scraped_list = list(scraped_accs['acc'])\n",
    "    \n",
    "    # for each account, scrape tweets containing hashtags if account retweeted one of the selected accounts\n",
    "    for a in range(len(accounts)):\n",
    "        acc = accounts[a]\n",
    "        if acc not in scraped_list:\n",
    "            try:\n",
    "                print('Scraping @' + acc)\n",
    "                profile_search_retweets(startdate, enddate, topic, acc, mentions_rt_dir, mentions_sel_dir, acclist, hstring)\n",
    "                scraped_list.append(acc)\n",
    "            except ValueError:\n",
    "                print('Account not found: @' + acc)\n",
    "                continue\n",
    "    \n",
    "    # update file listing scraped accounts\n",
    "    scraped_accs = pd.DataFrame(columns=['acc'])\n",
    "    scraped_accs['acc'] = scraped_list\n",
    "    scraped_accs.to_csv(topic_dir + 'scraped_accs.csv',encoding='utf-8',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_add_3(topic, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Combines the results of stages 1 to 3 of the bubble-generating procedure.\n",
    "    INPUTS: topic:            STRING, name of topic, used to name folders\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "\n",
    "    # set directories \n",
    "    topic_dir = directory + topic + '/'\n",
    "    main_dir = topic_dir + '01_main/'\n",
    "    retweets_dir = topic_dir + '02_retweets/'\n",
    "    mentions_dir = topic_dir + '03_mentions/'\n",
    "    mentions_rt_dir = mentions_dir + 'retweets/'\n",
    "    mentions_sel_dir = mentions_dir + 'select/'\n",
    "\n",
    "    # import combined dataset of stages 1 and 2\n",
    "    comb = pd.DataFrame(columns=['id','date','time','acc','tweet'])\n",
    "    comb = comb.append(pd.read_csv(topic_dir + 'combine_1_2.csv', encoding='utf-8'))\n",
    "    \n",
    "    # add data from stage 3\n",
    "    for index, filename in enumerate(os.listdir(mentions_sel_dir)):\n",
    "        if filename.endswith('.csv') and filename.startswith('select_'):\n",
    "            comb=comb.append(pd.read_csv(mentions_sel_dir + filename, encoding='utf-8'))\n",
    "    \n",
    "    # save combined dataset\n",
    "    comb.to_csv(topic_dir + 'combine_3.csv',encoding='utf-8',index=None)\n",
    "    print('Tweets found (steps 1-3): ' + str(len(comb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vague_clean(topic, file, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Vaguely cleaning the dataset.\n",
    "    INPUTS: topic:            STRING, name of topic, used to name folders\n",
    "            file:             STRING, name of file\n",
    "            directory:        STRING, directory\n",
    "    \"\"\"    \n",
    "\n",
    "    # set directories \n",
    "    topic_dir = directory + topic + '/'\n",
    "    mentions_dir = topic_dir + '03_mentions/'\n",
    "    mentions_rt_dir = mentions_dir + 'retweets/'\n",
    "    mentions_sel_dir = mentions_dir + 'select/'\n",
    "    \n",
    "    # import dataset\n",
    "    data = pd.read_csv(topic_dir + file + '.csv',encoding='utf-8')\n",
    "    print('Total tweets: ' + str(len(data)))\n",
    "    \n",
    "    # remove all tweets mostly consisting of hashtags, mentions, or links, as well as short tweets\n",
    "    print('Removing all hashtags, mentions, and links.')\n",
    "    for i in range(len(data)):\n",
    "        split = data.loc[i,'tweet'].split(' ')\n",
    "        cleaned_str = ''\n",
    "        for j in range(len(split)):\n",
    "            if split[j].startswith('@'): continue\n",
    "            if split[j].startswith('#'): continue\n",
    "            if split[j].startswith('http'): continue\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + split[j] + ' '\n",
    "        cleaned_str = cleaned_str.rstrip(' ')\n",
    "        if len(cleaned_str.split(' ')) > 2:\n",
    "            data.loc[i,'cleaned_tweet'] = cleaned_str\n",
    "    print('Removing duplicate and short tweets.')\n",
    "    data = data.drop_duplicates(subset=['cleaned_tweet'])\n",
    "    print('Remaining tweets: ' + str(len(data)))\n",
    "    \n",
    "    # save remaining tweets\n",
    "    subset = ['id', 'date', 'time', 'acc', 'tweet']\n",
    "    data = data[subset]\n",
    "    data.to_csv(topic_dir + file + '_no_duplicates.csv',encoding='utf-8',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(startdate, enddate, topic, file, directory, min_tweets):\n",
    "\n",
    "    \"\"\"\n",
    "    Scrapes the tweets of all accounts in the dataset.\n",
    "    INPUTS: startdate:        STRING, start date in yyyy-mm-dd format\n",
    "            enddate:          STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            file:             STRING, filename of dataset\n",
    "            directory:        STRING, directory\n",
    "            min_tweets:       INTEGER, number of tweets of an account in the dataset in order for other tweets of this\n",
    "                                       account to be considered as well\n",
    "    \"\"\"    \n",
    "    \n",
    "    # set directories     \n",
    "    topic_dir = directory + topic + '/'\n",
    "    mentions_dir = topic_dir + '03_mentions/'\n",
    "    mentions_rt_dir = mentions_dir + 'retweets/'\n",
    "    mentions_sel_dir = mentions_dir + 'select/'\n",
    "    expand_dir = topic_dir + '04_expand/'\n",
    "    \n",
    "    # create non-existent folder\n",
    "    if not os.path.exists(expand_dir): \n",
    "        os.makedirs(expand_dir)\n",
    "    \n",
    "    # import dataset\n",
    "    data = pd.read_csv(topic_dir + file + '.csv',encoding='utf-8')\n",
    "    \n",
    "    # generate list on which the dataset is expanded\n",
    "    counts = data['acc'].value_counts().rename_axis('acc').reset_index(name='count')\n",
    "    counts = counts.loc[counts['count']>min_tweets]\n",
    "    print('Expand on ' + str(len(counts)) + ' accounts.')\n",
    "    accounts = list(counts['acc'])\n",
    "    \n",
    "    # expand dataset\n",
    "    for i in range(len(accounts)):\n",
    "        account = accounts[i]\n",
    "        search_search(startdate, enddate, topic, account, expand_dir, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_expand(topic, directory, file=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Combines the results of stages 1 to 4 of the bubble-generating procedure.\n",
    "    INPUTS: topic:            STRING, name of topic, used to name folders\n",
    "            directory:        STRING, directory\n",
    "            file (optional):  STRING, filename of dataset\n",
    "    \"\"\"    \n",
    "\n",
    "    # set directories and import data\n",
    "    topic_dir = directory + topic + '/'\n",
    "    if file!=False:\n",
    "        data = pd.read_csv(topic_dir + file + '.csv',encoding='utf-8')\n",
    "        print('Total tweets: ' + str(len(data)))\n",
    "        expand_dir = topic_dir + '04_expand/'\n",
    "    else:\n",
    "        data = pd.DataFrame()\n",
    "        expand_dir = topic_dir + 'expand/'\n",
    "    \n",
    "    # add tweets of stage 4 to dataset\n",
    "    for index, filename in enumerate(os.listdir(expand_dir)):\n",
    "        if filename.endswith('.csv') and filename.startswith('select_'):\n",
    "            data=data.append(pd.read_csv(expand_dir + filename, encoding='utf-8'))\n",
    "    data=data.dropna(subset=['tweet'])\n",
    "    \n",
    "    # save dataset\n",
    "    data.to_csv(topic_dir + 'final.csv',encoding='utf-8',index=None)\n",
    "    print('Total tweets: ' + str(len(data)))\n",
    "    \n",
    "    # vaguely cleaning the dataset\n",
    "    vague_clean(topic, 'final', directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bubble(start, end, topic, main_account, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a bubble from a single Twitter account.\n",
    "    INPUTS: start:            STRING, start date in yyyy-mm-dd format\n",
    "            end:              STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            main_account:     STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "    \"\"\" \n",
    "    \n",
    "    # Stage 1\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 1: Main Account')\n",
    "    main_acc(start, end, topic, main_account, directory)\n",
    "    print('Stage 1 completed.')\n",
    "    \n",
    "    # Stage 2\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 2: Retweeted Accounts')\n",
    "    main_retweets(start, end, topic, main_account, directory)\n",
    "    print('Combining stage 1 and stage 2.')\n",
    "    combine_1_2(topic, main_account, directory)\n",
    "    print('Stage 2 completed.')\n",
    "    \n",
    "    # Stage 3\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 3: Mentions of Main Account')\n",
    "    main_mentions(start, end, topic, main_account, directory)\n",
    "    print('Combining stages 1 to 3.')\n",
    "    combine_add_3(topic, directory)\n",
    "    print('Cleaning...')\n",
    "    vague_clean(topic, 'combine_3', directory)\n",
    "    print('Stage 3 completed.')\n",
    "    \n",
    "    # Stage 4\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 4: Expand Tweets')\n",
    "    expand(start, end, topic, 'combine_3_no_duplicates', directory,0)\n",
    "    print('Combine and clean all data.')\n",
    "    combine_expand(topic, directory, 'combine_3_no_duplicates')\n",
    "    print('Bubble created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bubble_otherstart(start, end, topic, main_account, directory, otherstart):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a bubble where the first two stages include an other Twitter account.\n",
    "    INPUTS: start:            STRING, start date in yyyy-mm-dd format\n",
    "            end:              STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            main_account:     STRING, name of main account\n",
    "            directory:        STRING, directory\n",
    "            otherstart:       STRING, name of other account\n",
    "    \"\"\" \n",
    "    \n",
    "    # Stage 1\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 1: Main Account')\n",
    "    main_acc(start, end, topic, otherstart, directory)\n",
    "    print('Stage 1 completed.')\n",
    "    \n",
    "    # Stage 2\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 2: Retweeted Accounts')\n",
    "    main_retweets(start, end, topic, otherstart, directory)\n",
    "    print('Combining stage 1 and stage 2.')\n",
    "    combine_1_2(topic, otherstart, directory)\n",
    "    print('Stage 2 completed.')\n",
    "    \n",
    "    # Stage 3\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 3: Mentions of Main Account')\n",
    "    main_mentions(start, end, topic, main_account, directory)\n",
    "    print('Combining stages 1 to 3.')\n",
    "    combine_add_3(topic, directory)\n",
    "    print('Cleaning...')\n",
    "    vague_clean(topic, 'combine_3', directory)\n",
    "    print('Stage 3 completed.')\n",
    "    \n",
    "    # Stage 4\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 4: Expand Tweets')\n",
    "    expand(start, end, topic, 'combine_3_no_duplicates', directory,0)\n",
    "    print('Combine and clean all data.')\n",
    "    combine_expand(topic, directory, 'combine_3_no_duplicates')\n",
    "    print('Bubble created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oldbubble_newtime(start, end, topic, oldbubble_file, directory):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a bubble with the same accounts of another bubble in a different time frame.\n",
    "    INPUTS: start:            STRING, start date in yyyy-mm-dd format\n",
    "            end:              STRING, end date in yyyy-mm-dd format\n",
    "            topic:            STRING, name of topic, used to name folders\n",
    "            oldbubble_file:   STRING, filename of final dataset of the old bubble\n",
    "            directory:        STRING, directory\n",
    "    \"\"\" \n",
    "    \n",
    "    # set directories\n",
    "    topic_dir = directory + topic + '/'\n",
    "    expand_dir = topic_dir + 'expand/'\n",
    "    \n",
    "    # generate non-existing folders\n",
    "    if not os.path.exists(topic_dir): \n",
    "        os.makedirs(expand_dir)\n",
    "    if not os.path.exists(expand_dir): \n",
    "        os.makedirs(expand_dir)\n",
    "\n",
    "    # generate list of accounts\n",
    "    oldbubble = pd.read_csv(oldbubble_file)\n",
    "    accounts = list(dict.fromkeys(list(oldbubble['acc'])))\n",
    "    print('Expand on ' + str(len(accounts)) + ' accounts.')\n",
    "    \n",
    "    # scrape tweets of accounts in new time frame\n",
    "    for i in range(len(accounts)):\n",
    "        account = accounts[i]\n",
    "        search_search(start, end, topic, account, expand_dir, '')\n",
    "    \n",
    "    # combine results\n",
    "    combine_expand(topic, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reducing Language Elements and Creating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_split(tweet):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits a tweet on spaces, #, and @.\n",
    "    INPUT:  tweet:   STRING, tweet\n",
    "    OUTPUT: split:   LIST, split tweet\n",
    "    \"\"\" \n",
    "    \n",
    "    # split the tweet\n",
    "    s_re = re.split('( |#|@)', tweet)\n",
    "    split = []\n",
    "    keep = ''\n",
    "    for s in range(len(s_re)):\n",
    "        if s_re[s]==' ' or s_re[s]=='':\n",
    "            continue\n",
    "        elif s_re[s].startswith('@') or s_re[s].startswith('#'):\n",
    "            keep = s_re[s]\n",
    "        else:\n",
    "            if keep!='':\n",
    "                split.append(keep+s_re[s])\n",
    "                keep=''\n",
    "            else:\n",
    "                split.append(s_re[s])\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, joint_dir, character):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalizes a dataset of hashtags or account names.\n",
    "    INPUTS: df:            DATAFRAME, dataframe containing hashtags or mentions in column 'element'\n",
    "            joint_dir:     STRING, directory\n",
    "            character:     STRING, '#' or '@'\n",
    "    \"\"\" \n",
    "    \n",
    "    for h in range(len(df)):\n",
    "        \n",
    "        # split element on capital letters\n",
    "        cap_split = re.findall('[A-Z][^A-Z]*', df.loc[h,'element'])\n",
    "        \n",
    "        if cap_split != []:\n",
    "            \n",
    "            # check for single letters after split and combine them\n",
    "            cap_split_checked = []\n",
    "            cap_split_singleletters_dump = ''\n",
    "            for e in range(len(cap_split)):\n",
    "                if len(''.join([i for i in cap_split[e].translate(str.maketrans('', '', string.punctuation)) if not i.isdigit()]))>1:\n",
    "                    if cap_split_singleletters_dump != '':\n",
    "                        cap_split_checked.append(cap_split_singleletters_dump)\n",
    "                        cap_split_singleletters_dump = ''\n",
    "                    cap_split_checked.append(cap_split[e])\n",
    "                else:\n",
    "                    cap_split_singleletters_dump = cap_split_singleletters_dump + cap_split[e]\n",
    "            if cap_split_singleletters_dump != '':\n",
    "                cap_split_checked.append(cap_split_singleletters_dump)\n",
    "            \n",
    "            # combine split element to string with spaces\n",
    "            cap_split_str = ''\n",
    "            characters = 0\n",
    "            if len(cap_split_checked)>1:\n",
    "                for i in range(len(cap_split_checked)):\n",
    "                    cap_split_str = cap_split_str + cap_split_checked[i] + ' '\n",
    "                    characters = characters + len(cap_split_checked[i])\n",
    "                if characters == len(df.loc[h,'element'])-1:\n",
    "                    df.loc[h,'cleaned']=cap_split_str.rstrip(' ')\n",
    "        stripped = df.loc[h,'element'].lstrip(character)\n",
    "        \n",
    "        # mark account name elements\n",
    "        if character=='@':\n",
    "            stripped = 'M_' + stripped\n",
    "        \n",
    "        # enter cleaned element in dataframe\n",
    "        if pd.isnull(df.loc[h,'cleaned']):\n",
    "            df.loc[h,'cleaned']=stripped\n",
    "\n",
    "    # save dataframe\n",
    "    df.to_excel(joint_dir + character + '.xlsx',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_hashtags_mentions(data, topic, twint_dir, joint_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Collects all hashtags and mentions of a dataset of tweets.\n",
    "    INPUTS:  data:          DATAFRAME, dataframe containing tweets\n",
    "             topic:         STRING, name of dataframe topic\n",
    "             twint_dir:     STRING, directory of results from TWINT scraping\n",
    "             joint_dir:     STRING, directory of event analysis\n",
    "    OUTPUTS: hashtags:      LIST, contains all hashtags of data\n",
    "             mentions:      LIST, contains all mentions of data\n",
    "    \"\"\" \n",
    "    \n",
    "    # initialize empty lists\n",
    "    hashtags = []\n",
    "    mentions = []\n",
    "    \n",
    "    # drop tweets with no content\n",
    "    data = data.dropna(subset=['tweet']).reset_index(drop=True)\n",
    "    \n",
    "    for tweet in range(len(data)):\n",
    "        \n",
    "        # split tweet\n",
    "        split = tweet_split(data.loc[tweet,'tweet'])\n",
    "        \n",
    "        # remove all mentions and hashtags at the end of the tweet\n",
    "        cont = True\n",
    "        while cont==True:\n",
    "            if len(split)>0:\n",
    "                if split[len(split)-1].startswith('#') or split[len(split)-1].startswith('@') or split[len(split)-1].startswith('http'):\n",
    "                    del split[len(split)-1]\n",
    "                else:\n",
    "                    cont=False\n",
    "            else:\n",
    "                cont=False\n",
    "        \n",
    "        # append hashtags and mentions to lists\n",
    "        for element in range(len(split)):\n",
    "            if split[element].startswith('#'):\n",
    "                hashtags.append(split[element])\n",
    "            if split[element].startswith('@'):\n",
    "                mentions.append(split[element])\n",
    "    \n",
    "    # generate and save dataframes\n",
    "    hashtags_df = pd.DataFrame(hashtags).value_counts().rename_axis('element').reset_index(name='count')\n",
    "    mentions_df = pd.DataFrame(mentions).value_counts().rename_axis('element').reset_index(name='count')\n",
    "    hashtags_df.to_csv(joint_dir+topic+'_#.csv',index=False)\n",
    "    mentions_df.to_csv(joint_dir+topic+'_@.csv',index=False)\n",
    "    return hashtags, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags_mentions(twint_dir, joint_dir, topic0, topic1):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates dataset listing hashtags and mentions and suggests normalizations.\n",
    "    INPUTS:  twint_dir:     STRING, directory of results from TWINT scraping\n",
    "             joint_dir:     STRING, directory of event analysis\n",
    "             topic0:        STRING, name of topic related to extreme 0\n",
    "             topic1:        STRING, name of topic related to extreme 1\n",
    "    \"\"\" \n",
    "    \n",
    "    # create non-existing folder\n",
    "    if not os.path.exists(joint_dir): \n",
    "        os.makedirs(joint_dir)\n",
    "        \n",
    "    # import data\n",
    "    data0 = pd.read_csv(twint_dir+ '/' + topic0 + '/final_no_duplicates.csv')\n",
    "    data1 = pd.read_csv(twint_dir+ '/' + topic1 + '/final_no_duplicates.csv')\n",
    "    \n",
    "    # collect hashtags and mentions of both datasets\n",
    "    hashtags0, mentions0 = collect_hashtags_mentions(data0, topic0, twint_dir, joint_dir)\n",
    "    hashtags1, mentions1 = collect_hashtags_mentions(data1, topic1, twint_dir, joint_dir)\n",
    "    \n",
    "    # combine lists of mentions and hashtags for both datasets\n",
    "    for h in range(len(hashtags1)):\n",
    "        hashtags0.append(hashtags1[h])\n",
    "    hashtags = hashtags0\n",
    "    for m in range(len(mentions1)):\n",
    "        mentions0.append(mentions1[m])\n",
    "    mentions = mentions0\n",
    "    \n",
    "    # count values in hashtag- and mention-list, add empty column 'cleaned'\n",
    "    hashtags_df = pd.DataFrame(hashtags).value_counts().rename_axis('element').reset_index(name='count')\n",
    "    mentions_df = pd.DataFrame(mentions).value_counts().rename_axis('element').reset_index(name='count')    \n",
    "    hashtags_df['cleaned'] = np.nan\n",
    "    mentions_df['cleaned'] = np.nan\n",
    "    \n",
    "    # normalize hashtags and mentions\n",
    "    normalize(hashtags_df, joint_dir, '#')\n",
    "    normalize(mentions_df, joint_dir, '@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(own, a):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates list of stop words from nltk-package and includes list of own stop words.\n",
    "    INPUTS:  own:     LIST, own stop words\n",
    "             a:       STRING, special character '’'\n",
    "    OUTPUT:  stop_np: LIST, stop words\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of stop words from nltk-package \n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    # replace special character for each element\n",
    "    stop_np = []\n",
    "    for i in range(len(stop)):\n",
    "        stop_np.append(stop[i].replace(a,\"'\").translate(str.maketrans('', '', string.punctuation)))\n",
    "        \n",
    "    # append own stop words\n",
    "    append = own\n",
    "    for i in range(len(append)):\n",
    "        stop_np.append(append[i])\n",
    "        \n",
    "    # return stop word list\n",
    "    return stop_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct1_rep_hashtags_mentions(data, joint_dir, a, b, c):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes emojis, punctuation, links, mark mentions, normalizes hashtags, and sets everything to lower-case letters.\n",
    "    INPUTS:  data:         DATAFRAME, contains tweets\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'\n",
    "    OUTPUT:  data:         DATAFRAME, contains edited tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # import hashtag normalization suggestions\n",
    "    h_rep = pd.read_excel(joint_dir + '#.xlsx')\n",
    "    \n",
    "    new_tweet = ''\n",
    "    for tweet in range(len(data)):\n",
    "        \n",
    "        # split tweet, remove emojis, remove special characters\n",
    "        split = tweet_split(data.loc[tweet,'tweet'].encode('ascii', 'ignore').decode('ascii').replace(a,\"\").replace(b,\"\").replace(c,\"\"))\n",
    "        \n",
    "        # remove all mentions and hashtags at the end of the tweet\n",
    "        cont = True\n",
    "        while cont==True:\n",
    "            if len(split)>0:\n",
    "                if split[len(split)-1].startswith('#') or split[len(split)-1].startswith('@') or split[len(split)-1].startswith('http'):\n",
    "                    del split[len(split)-1]\n",
    "                else:\n",
    "                    cont=False\n",
    "            else:\n",
    "                cont=False\n",
    "                \n",
    "        # remove hashtags, links, and punctuation; replace mentions\n",
    "        if len(split)>0:\n",
    "            new_tweet = ''\n",
    "            for element in range(len(split)):\n",
    "                if split[element].startswith('#'):\n",
    "                    h_rep_x = h_rep.loc[h_rep['element'].str.lower()==split[element].lower()].reset_index(drop=True)\n",
    "                    if len(h_rep_x)>0:\n",
    "                        new_tweet = new_tweet + str(h_rep_x.loc[0,'cleaned'].translate(str.maketrans('', '', string.punctuation))) + ' '\n",
    "                    else:\n",
    "                        new_tweet = new_tweet + 'h_' + split[element].translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "                elif split[element].startswith('@'):\n",
    "                    new_tweet = new_tweet + 'm_' + split[element].translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "                    continue\n",
    "                elif split[element].startswith('http'):\n",
    "                    continue\n",
    "                else:\n",
    "                    new_tweet = new_tweet + split[element].translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "            \n",
    "            # save edited tweet and set to lower-case letters\n",
    "            data.loc[tweet,'tweet'] = new_tweet.lower().rstrip(' ')\n",
    "        else:\n",
    "            data.loc[tweet,'tweet'] = np.nan\n",
    "    \n",
    "    # drop empty tweets\n",
    "    data = data.dropna(subset=['tweet']).reset_index(drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct2_mcn_random(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Performs Multiple Character Normalization (MCN), and removes random letter sequences.\n",
    "    INPUT:   data:         DATAFRAME, contains tweets\n",
    "    OUTPUT:  data:         DATAFRAME, contains edited tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize spell checker\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    for tweet in range(len(data)):\n",
    "        \n",
    "        # split tweet\n",
    "        split = tweet_split(data.loc[tweet,'tweet'])\n",
    "        \n",
    "        new_tweet = ''\n",
    "        for element in range(len(split)):\n",
    "            \n",
    "            # pass mentions\n",
    "            if split[element].startswith('m_'):\n",
    "                new_tweet = new_tweet + split[element] + ' '\n",
    "            \n",
    "            # multiple character normalization\n",
    "            else:\n",
    "                replaced = re.sub(r'(.)\\1{2,}', r'\\1', split[element])\n",
    "                if len(replaced)>0:\n",
    "                    if len(replaced)/len(set(replaced))>4:\n",
    "                        continue\n",
    "                    else:\n",
    "                        \n",
    "                        # apply spell chacker to replaced elements\n",
    "                        if split[element]!=replaced:\n",
    "                            new_tweet = new_tweet + spell.correction(replaced).translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "                        else:\n",
    "                            new_tweet = new_tweet + replaced + ' '\n",
    "                else:\n",
    "                    new_tweet = new_tweet + replaced + ' '\n",
    "        \n",
    "        # save edited tweet\n",
    "        data.loc[tweet,'tweet'] = new_tweet.rstrip(' ')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct3_stop_stem(data, a):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes stop words and applies the Porter stemmer.\n",
    "    INPUTS:  data:         DATAFRAME, contains tweets\n",
    "             a:            STRING, special character '’'\n",
    "    OUTPUT:  data:         DATAFRAME, contains edited tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # create list of stop words\n",
    "    own_stopwords = ['cannot', 'gonna', 'gotta', 'im', 'ive', 'like', 'cant', 'whats', 'wanna', 'us', 'amp', 'lets', 'gimme', 'gimmee']\n",
    "    \n",
    "    # create complete list of stop words\n",
    "    stop_np = stop_words(own_stopwords, a)\n",
    "    \n",
    "    for tweet in range(len(data)):\n",
    "        \n",
    "        # split tweet\n",
    "        split = tweet_split(data.loc[tweet,'tweet'])\n",
    "        new_tweet = ''\n",
    "        for element in range(len(split)):\n",
    "            \n",
    "            # remove stop words\n",
    "            if split[element] in stop_np:\n",
    "                continue\n",
    "            \n",
    "            # pass mentions\n",
    "            elif split[element].startswith('m_'):\n",
    "                new_tweet = new_tweet + split[element] + ' '\n",
    "            \n",
    "            # stem remaining elements\n",
    "            else:\n",
    "                new_tweet = new_tweet + stemmer.stem(split[element]) + ' '\n",
    "        \n",
    "        # save edited tweet\n",
    "        data.loc[tweet,'tweet'] = new_tweet.rstrip(' ')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(twint_dir, joint_dir, topic, a, b, c):\n",
    "\n",
    "    \"\"\"\n",
    "    Cleans the tweets.\n",
    "    INPUTS:  twint_dir:    STRING, directory of results from TWINT scraping\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             topic:        STRING, name of topic\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'\n",
    "    OUTPUT:  data:         DATAFRAME, contains edited tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # import data\n",
    "    data = pd.read_csv(twint_dir+ '/' + topic + '/final_no_duplicates.csv').dropna(subset=['tweet']).reset_index(drop=True)\n",
    "    \n",
    "    # stage 1a\n",
    "    print('Stage 1a: Cleaning hashtags and mentions; removing emojis, links, punctuation, and capitalizations...')\n",
    "    data = ct1_rep_hashtags_mentions(data, joint_dir, a, b, c)\n",
    "    print('Stage 1a completed.')\n",
    "    \n",
    "    # stage 1b\n",
    "    print('Stage 1b: MCN and removing random letter sequences...')\n",
    "    data = ct2_mcn_random(data)\n",
    "    data.to_csv(joint_dir+topic+'_cleaned.csv',index=False)\n",
    "    print('Stage 1b completed.')\n",
    "    \n",
    "    # stage 1c\n",
    "    print('Stage 1c: Removing stop words and applying the Porter stemmer...')\n",
    "    data = ct3_stop_stem(data, a)\n",
    "    print('Stage 1c completed.')\n",
    "    \n",
    "    # save\n",
    "    data.to_csv(joint_dir+topic+'_stemmed.csv',index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(df, column, n, joint_dir, topic):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates n-grams.\n",
    "    INPUTS:  df:           DATAFRAME, contains data\n",
    "             column:       STRING, indicates column of df for which n-grams should be generated\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             topic:        STRING, name of topic\n",
    "    \"\"\"\n",
    "    \n",
    "    # drops empty entries\n",
    "    df = df.dropna().reset_index(drop=True) # DELETE!!!\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # bigrams\n",
    "        if n==2:\n",
    "            i_ngrams = list(nltk.bigrams(nltk.word_tokenize(df.loc[i,column])))\n",
    "        \n",
    "        # trigrams\n",
    "        if n==3:\n",
    "            i_ngrams = list(nltk.trigrams(nltk.word_tokenize(df.loc[i,column])))\n",
    "        \n",
    "        # generate list if n-grams\n",
    "        for j in range(len(i_ngrams)):\n",
    "            ngrams.append(i_ngrams[j])\n",
    "    \n",
    "    # generate dataframe of n-grams\n",
    "    x = pd.DataFrame()\n",
    "    x['ngrams'] = ngrams\n",
    "    \n",
    "    # count n-grams\n",
    "    ngramsdf = x['ngrams'].value_counts().rename_axis('ngrams').reset_index(name='count')\n",
    "    \n",
    "    # save\n",
    "    if n==2:\n",
    "        ngramsdf.to_csv(joint_dir+topic+'_2grams.csv',index=False)\n",
    "    if n==3:\n",
    "        ngramsdf.to_csv(joint_dir+topic+'_3grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(twint_dir, topic, joint_dir, n, a, b, c):\n",
    "\n",
    "    \"\"\"\n",
    "    Cleans the tweets and generates n-grams.\n",
    "    INPUTS:  twint_dir:    STRING, directory of results from TWINT scraping\n",
    "             topic:        STRING, name of topic\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'        \n",
    "    \"\"\"\n",
    "    \n",
    "    # stage 1\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 1: Cleaning the Tweets')\n",
    "    if not os.path.exists(joint_dir+topic+'_stemmed.csv'):\n",
    "        tweets_cleaned = clean_tweets(twint_dir, joint_dir, topic, a, b, c)\n",
    "    else:\n",
    "        tweets_cleaned = pd.read_csv(joint_dir+topic+'_stemmed.csv')\n",
    "    print('Stage 1 completed.')\n",
    "    \n",
    "    # stage 2\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 2: Creating N-grams')\n",
    "    make_ngrams(tweets_cleaned, 'tweet', n, joint_dir, topic)\n",
    "    print('Stage 2 completed.')\n",
    "    print('N-grams created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_to_phrases(n, joint_dir, topic, data, tweets, column):\n",
    "\n",
    "    \"\"\"\n",
    "    Finding phrases belonging to n-grams.\n",
    "    INPUTS:  n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             topic:        STRING, name of topic\n",
    "             data:         DATAFRAME, containing cleaned tweets\n",
    "             tweets:       DATAFRAME, containing stemmed tweets\n",
    "             column:       STRING, indicates column where tweets are saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # create phrases-dataframe\n",
    "    phrases = pd.DataFrame(columns=['ngram', 'phrase'])\n",
    "    \n",
    "    # create non-existend folder\n",
    "    if not os.path.exists(joint_dir+topic+'_'+str(n)+'_phrases_build/'): \n",
    "        os.makedirs(joint_dir+topic+'_'+str(n)+'_phrases_build/')\n",
    "    \n",
    "    for tweet in range(len(tweets)):\n",
    "        i=0\n",
    "        if type(tweets.loc[tweet, column])==str: # if there is a tweet with only removed words, this tweet has to be skipped\n",
    "            try:\n",
    "                \n",
    "                # generate n-grams\n",
    "                if n==2:\n",
    "                    i_ngrams=list(nltk.bigrams(nltk.word_tokenize(tweets.loc[tweet,column])))\n",
    "                if n==3:\n",
    "                    i_ngrams=list(nltk.trigrams(nltk.word_tokenize(tweets.loc[tweet,column])))\n",
    "                \n",
    "                # split cleaned tweet\n",
    "                l=data.loc[tweet,column].split(' ')\n",
    "                \n",
    "                for j in range(len(i_ngrams)):\n",
    "                    \n",
    "                    # skip over marked elements\n",
    "                    if n==2 and (i_ngrams[j][0].startswith('m_') or i_ngrams[j][0].startswith('h_') or i_ngrams[j][1].startswith('m_') or i_ngrams[j][1].startswith('h_')):\n",
    "                        continue\n",
    "                    elif n==3 and (i_ngrams[j][0].startswith('m_') or i_ngrams[j][0].startswith('h_') or i_ngrams[j][1].startswith('m_') or i_ngrams[j][1].startswith('h_') or i_ngrams[j][2].startswith('m_') or i_ngrams[j][2].startswith('h_')):\n",
    "                        continue\n",
    "                    \n",
    "                    # find start and end element of cleaned tweet relating to n-gram\n",
    "                    else:\n",
    "                        foundstart=0\n",
    "                        foundend=0\n",
    "                        phrase=''\n",
    "                        \n",
    "                        # find start\n",
    "                        while foundstart==0:\n",
    "                            if stemmer.stem(l[i])==i_ngrams[j][0]:\n",
    "                                phrase=phrase+l[i]\n",
    "                                foundstart=1\n",
    "                                i_save=i+1\n",
    "                                i=i_save\n",
    "                            else:\n",
    "                                i=i+1\n",
    "                        \n",
    "                        # find end\n",
    "                        while foundend==0:\n",
    "                            if n==2:\n",
    "                                if stemmer.stem(l[i])==i_ngrams[j][1]:\n",
    "                                    phrase=phrase+' '+l[i]\n",
    "                                    foundend=1\n",
    "                                else:\n",
    "                                    phrase=phrase+' '+l[i]\n",
    "                                    i=i+1\n",
    "                            if n==3:\n",
    "                                if stemmer.stem(l[i])==i_ngrams[j][2]:\n",
    "                                    phrase=phrase+' '+l[i]\n",
    "                                    foundend=1\n",
    "                                    i=i_save\n",
    "                                else:\n",
    "                                    phrase=phrase+' '+l[i]\n",
    "                                    i=i+1                            \n",
    "                        \n",
    "                        # save phrase\n",
    "                        phrase=phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "                        phrase=' '.join(phrase.rstrip(' ').split())\n",
    "                        row=len(phrases)\n",
    "                        phrases.loc[row,'ngram']=i_ngrams[j]\n",
    "                        phrases.loc[row,'phrase']=phrase\n",
    "            except:\n",
    "                print(tweet)\n",
    "                continue\n",
    "        \n",
    "        # save every 1000 tweets and at the end\n",
    "        if tweet % 1000 == 0:\n",
    "            phrases.to_csv(joint_dir+topic+'_'+str(n)+'_phrases_build/phr'+str(int(tweet/1000))+'.csv',index=False)\n",
    "            phrases = pd.DataFrame(columns=['ngram', 'phrase'])\n",
    "        elif tweet == len(tweets)-1:\n",
    "            phrases.to_csv(joint_dir+topic+'_'+str(n)+'_phrases_build/phr'+str(int(tweet/1000)+1)+'.csv',index=False)\n",
    "\n",
    "    # merge individual saves, save final phrases document\n",
    "    for index, filename in enumerate(os.listdir(joint_dir+topic+'_'+str(n)+'_phrases_build/')):\n",
    "        phrases=phrases.append(pd.read_csv(joint_dir+topic+'_'+str(n)+'_phrases_build/'+filename))\n",
    "    phrases = phrases.reset_index(drop=True)\n",
    "    phrases=phrases.groupby(['ngram','phrase']).size().reset_index(name='count').sort_values(by='count',ascending=False).reset_index(drop=True)\n",
    "    phrases.to_csv(joint_dir+topic+'_phrases_'+str(n)+'grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phrases(joint_dir, topic, n):\n",
    "\n",
    "    \"\"\"\n",
    "    Finding phrases belonging to n-grams; easier for inputs.\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             topic:        STRING, name of topic\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import data\n",
    "    data = pd.read_csv(joint_dir + topic + '_cleaned.csv')\n",
    "    tweets = pd.read_csv(joint_dir + topic + '_stemmed.csv')\n",
    "    \n",
    "    # call function ngrams_to_phrases\n",
    "    ngrams_to_phrases(n, joint_dir, topic, data, tweets, 'tweet')\n",
    "    \n",
    "    print('Phrases computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(topic0, topic1, joint_dir, n):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute chi2-value for every phrase.\n",
    "    INPUTS:  topic0:       STRING, name of topic relating to extreme 0\n",
    "             topic1:       STRING, name of topic relating to extreme 1\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "    OUTPUT:  counts:       DATAFRAME, contains chi2-values\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up counts-dataframe containing combined n-gram counts\n",
    "    data1 = pd.read_csv(joint_dir+topic1+'_'+str(n)+'grams.csv').rename(columns={'count': 'count_1'})\n",
    "    data0 = pd.read_csv(joint_dir+topic0+'_'+str(n)+'grams.csv').rename(columns={'count': 'count_0'})\n",
    "    counts = pd.merge(data0, data1, on='ngrams',how='outer').fillna(0)\n",
    "    counts['count_comb'] = counts['count_1'] + counts['count_0']\n",
    "    counts = counts.sort_values(by='count_comb',ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # reduce to highest n-gram counts\n",
    "    value = counts.loc[9999,'count_comb']\n",
    "    counts = counts.loc[counts['count_comb']>=value]\n",
    "    \n",
    "    # compute values of total n-gram counts per extreme\n",
    "    total_1 = sum(counts['count_1'])\n",
    "    total_0 = sum(counts['count_0'])\n",
    "    \n",
    "    # calculate chi2-values\n",
    "    for i in range(len(counts)):\n",
    "        fp0 = counts.loc[i,'count_0']\n",
    "        fp1 = counts.loc[i,'count_1']\n",
    "        fnp0 = total_0 - fp0\n",
    "        fnp1 = total_1 - fp1\n",
    "        num = ((fp0*fnp1)-(fp1*fnp0))**2\n",
    "        denom = (fp0+fp1)*(fp0+fnp0)*(fp1+fnp1)*(fnp0+fnp1)\n",
    "        chi2 = num / denom\n",
    "        counts.loc[i,'chi2'] = chi2\n",
    "    \n",
    "    # sort by chi2-values and drop n-grams containing elements starting with h_ or m_\n",
    "    counts = counts.sort_values(by='chi2',ascending=False).reset_index(drop=True)\n",
    "    counts = counts.loc[~counts['ngrams'].str.contains('h_|m_')].reset_index(drop=True)\n",
    "    \n",
    "    # drop all n-grams containing elements with only 1 or 2 characters\n",
    "    for i in range(len(counts)):\n",
    "        split = counts.loc[i,'ngrams'].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        short = 0\n",
    "        for element in range(len(split)):\n",
    "            if len(split[element])<3:\n",
    "                short = 1\n",
    "        if short==1:\n",
    "            counts=counts.drop([i])\n",
    "    \n",
    "    # save and return\n",
    "    counts = counts.reset_index(drop=True)\n",
    "    counts.to_csv(joint_dir+'chi2_test_'+str(n)+'grams.csv',index=False)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_counts(counts, topic0, topic1, joint_dir, n, select, a, b, c):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects a given number of n-grams/phrases based on chi2-values.\n",
    "    INPUTS:  counts:       DATAFRAME, contains chi2-values\n",
    "             topic0:       STRING, name of topic relating to extreme 0\n",
    "             topic1:       STRING, name of topic relating to extreme 1\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "             select:       INTEGER, defines number of n-grams/phrases to be selected.\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'  \n",
    "    \"\"\"    \n",
    "    \n",
    "    # import and combine phrases\n",
    "    phrases0 = pd.read_csv(joint_dir + topic0 + '_phrases_' + str(n) + 'grams.csv')\n",
    "    phrases1 = pd.read_csv(joint_dir + topic1 + '_phrases_' + str(n) + 'grams.csv')\n",
    "    phrases = phrases0.append(phrases1).reset_index(drop=True)\n",
    "    phrases = phrases.rename(columns={'count': 'counts'})\n",
    "    phrases = phrases.groupby(['ngram','phrase']).counts.sum().reset_index().sort_values(by='counts',ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # select top 2000 most used n-grams, find phrases belonging to n-grams\n",
    "    selected_counts = counts[0:2000]\n",
    "    for i in range(len(selected_counts)):\n",
    "        ngram=selected_counts.loc[i,'ngrams']\n",
    "        phrases_x=phrases.loc[phrases['ngram']==ngram].sort_values(by='counts',ascending=False).reset_index(drop=True)\n",
    "        if len(phrases_x.loc[0,'phrase'].replace(' ',''))>3:\n",
    "            selected_counts.loc[i,'phrase_freq_1']=phrases_x.loc[0,'phrase']\n",
    "            if len(phrases_x)>1:\n",
    "                # second phrase has to be used more than 5 times to be considered\n",
    "                if phrases_x.loc[1,'counts']>5:\n",
    "                    if len(phrases_x.loc[1,'phrase'].replace(' ',''))>3:\n",
    "                        selected_counts.loc[i,'phrase_freq_2']=phrases_x.loc[1,'phrase']\n",
    "    \n",
    "    #compute hashtag ratio and delete hashtag-dominated n-grams (same as in clean_tweets)\n",
    "    hashtags = pd.read_excel(joint_dir+'#.xlsx')\n",
    "    hashtags.columns = ['element', 'count', 'tweet']\n",
    "    for tweet in range(len(hashtags)):\n",
    "        split = tweet_split(hashtags.loc[tweet,'tweet'].encode('ascii', 'ignore').decode('ascii').replace(a,\"\").replace(b,\"\").replace(c,\"\"))\n",
    "        new_tweet = ''\n",
    "        for element in range(len(split)):\n",
    "            new_tweet = new_tweet + split[element].translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "        hashtags.loc[tweet,'tweet'] = new_tweet.lower().rstrip(' ')\n",
    "    spell = SpellChecker()\n",
    "    for tweet in range(len(hashtags)):\n",
    "        split = tweet_split(hashtags.loc[tweet,'tweet'])\n",
    "        new_tweet = ''\n",
    "        for element in range(len(split)):\n",
    "            replaced = re.sub(r'(.)\\1{2,}', r'\\1', split[element])\n",
    "            if len(replaced)>0:\n",
    "                if len(replaced)/len(set(replaced))>4:\n",
    "                    continue\n",
    "                else:\n",
    "                    if split[element]!=replaced:\n",
    "                        new_tweet = new_tweet + spell.correction(replaced).translate(str.maketrans('', '', string.punctuation)) + ' '\n",
    "                    else:\n",
    "                        new_tweet = new_tweet + replaced + ' '\n",
    "            hashtags.loc[tweet,'tweet'] = new_tweet.lower().rstrip(' ')\n",
    "    stemmer = PorterStemmer()\n",
    "    own_stopwords = ['cannot', 'gonna', 'gotta', 'im', 'ive', 'like', 'cant', 'whats', 'wanna', 'us', 'amp', 'lets', 'gimme', 'gimmee']\n",
    "    stop_np = stop_words(own_stopwords, a)\n",
    "    for tweet in range(len(hashtags)):\n",
    "        split = tweet_split(hashtags.loc[tweet,'tweet'])\n",
    "        new_tweet = ''\n",
    "        for element in range(len(split)):\n",
    "            if split[element] in stop_np:\n",
    "                continue\n",
    "            else:\n",
    "                new_tweet = new_tweet + stemmer.stem(split[element]) + ' '\n",
    "        hashtags.loc[tweet,'tweet'] = new_tweet.rstrip(' ')\n",
    "    for i in range(len(hashtags)):\n",
    "        if n==2:\n",
    "            i_ngrams = list(nltk.bigrams(nltk.word_tokenize(hashtags.loc[i,'tweet'])))\n",
    "        if n==3:\n",
    "            i_ngrams = list(nltk.trigrams(nltk.word_tokenize(hashtags.loc[i,'tweet'])))\n",
    "        for j in range(len(i_ngrams)):\n",
    "            hashtags.loc[i,'ngram'+str(j)] = str(i_ngrams[j])\n",
    "    for i in range(len(selected_counts)):\n",
    "        hashtags_x = hashtags[hashtags.eq(selected_counts.loc[i,'ngrams']).any(1)]\n",
    "        selected_counts.loc[i,'hashtag_ratio'] = sum(hashtags_x['count'])/selected_counts.loc[i,'count_comb']\n",
    "    selected_counts=selected_counts.loc[selected_counts['hashtag_ratio']<0.2].reset_index(drop=True)\n",
    "    \n",
    "    # select equal amount of ngrams that are more likely to be used by one extreme\n",
    "    tweets0 = pd.read_csv(joint_dir + topic0 + '_stemmed.csv')\n",
    "    tweets1 = pd.read_csv(joint_dir + topic1 + '_stemmed.csv')\n",
    "    selected_counts['most_likely'] = (selected_counts['count_0']/len(tweets0) < selected_counts['count_1']/len(tweets1)).astype(int)\n",
    "    select0 = selected_counts.loc[selected_counts['most_likely']==0].sort_values(by='chi2',ascending=False).reset_index(drop=True)[0:int(select/2)]\n",
    "    select1 = selected_counts.loc[selected_counts['most_likely']==1].sort_values(by='chi2',ascending=False).reset_index(drop=True)[0:int(select/2)]\n",
    "    comb = select0.append(select1).reset_index(drop=True)\n",
    "    \n",
    "    # save selected n-grams\n",
    "    comb.to_csv(joint_dir+'selected_counts_'+str(n)+'grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_and_select(topic0, topic1, joint_dir, n, select, a, b, c):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes chi2-values and selects a given number of n-grams/phrases based on chi2-values.\n",
    "    INPUTS:  topic0:       STRING, name of topic relating to extreme 0\n",
    "             topic1:       STRING, name of topic relating to extreme 1\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "             select:       INTEGER, defines number of n-grams/phrases to be selected.\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'  \n",
    "    \"\"\"    \n",
    "    \n",
    "    # stage 1\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 1: Chi-Squared Test')\n",
    "    counts = chi_squared(topic0, topic1, joint_dir, n)\n",
    "    print('Stage 1 completed.')\n",
    "    \n",
    "    # stage 2\n",
    "    print('---------------------------------------------')\n",
    "    print('Stage 2: Select phrases.')\n",
    "    selected_counts(counts, topic0, topic1, joint_dir, n, select, a, b, c)\n",
    "    print('Stage 2 completed.')\n",
    "    print('Phrases selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_articles(domain, start, end, gdelt_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Finds all articles of a given outlet in a given time frame.\n",
    "    INPUTS:  domain:      STRING, domain\n",
    "             start:       STRING, start date in yymmdd-format\n",
    "             end:         STRING, end date in yymmdd-format\n",
    "             gdelt_dir:   STRING, directory where GDELT-results are saved\n",
    "    \"\"\"    \n",
    "    \n",
    "    # define base url for GDELT-API\n",
    "    base='https://api.gdeltproject.org/api/v2/doc/doc?query='\n",
    "    \n",
    "    # convert dates\n",
    "    start_dt=datetime.datetime.strptime(start,\"%y%m%d\")\n",
    "    end_dt=datetime.datetime.strptime(end,\"%y%m%d\")\n",
    "    iter_dt=start_dt\n",
    "    \n",
    "    # initialize articles-dataframe\n",
    "    cols=['url','title','seendate','domain']\n",
    "    articles=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    # iterate in steps of 1 hour\n",
    "    while iter_dt+timedelta(hours=1)<=end_dt:\n",
    "        \n",
    "        # define start and end time for current search\n",
    "        cur_start=str(iter_dt).replace('-','').replace(':','').replace(' ','')\n",
    "        cur_end=str(iter_dt+timedelta(hours=1)).replace('-','').replace(':','').replace(' ','')\n",
    "        \n",
    "        # generate url\n",
    "        url=base+'domainis:'+domain+'&sourcelang:english&sourcecountry:us&format=json&STARTDATETIME='+cur_start+'&ENDDATETIME='+cur_end+'&MAXRECORDS=250'\n",
    "        \n",
    "        # start time\n",
    "        starttime=time.time()\n",
    "        \n",
    "        # visit url, read json, append results to articles\n",
    "        r=requests.get(url)\n",
    "        j=r.text.lstrip('{\"articles\": ').rstrip(' }')\n",
    "        if j!='':\n",
    "            results = pd.read_json(StringIO(j))\n",
    "            articles=articles.append(results[cols]).reset_index(drop=True)\n",
    "            print(cur_start+': '+str(len(results))+' articles found. Total articles: '+str(len(articles))+'.')\n",
    "        else:\n",
    "            print(cur_start+': No new articles found.')\n",
    "        iter_dt=iter_dt+timedelta(hours=1)\n",
    "        \n",
    "        # check elapsed time, if less than 5 seconds: implement a delay\n",
    "        elapsedtime=time.time()-starttime\n",
    "        if elapsedtime<5:\n",
    "            time.sleep(5-elapsedtime)\n",
    "    \n",
    "    # drop duplicate articles\n",
    "    n_before=len(articles)\n",
    "    articles=articles.drop_duplicates(subset=['url'])\n",
    "    print('Deleted '+str(n_before-len(articles))+' articles. '+str(len(articles))+' remaining.')\n",
    "    \n",
    "    # generate non-existent folder\n",
    "    if not os.path.exists(gdelt_dir+'domains/'): \n",
    "        os.makedirs(gdelt_dir+'domains/')\n",
    "    \n",
    "    # save articles\n",
    "    articles.to_csv(gdelt_dir+'domains/'+domain.split('.')[0]+'_'+start+'_'+end+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_phrase(domain, phrase, start, end, phrase2=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Finds all articles of a given outlet in a given time frame containing one or two phrases.\n",
    "    INPUTS:  domain:             STRING, domain\n",
    "             phrase:             STRING, phrase 1 that has to be mentioned for an article to be found\n",
    "             start:              STRING, start date in yymmdd-format\n",
    "             end:                STRING, end date in yymmdd-format\n",
    "             phrase2 (optional)  STRING, phrase 2 that has to be mentioned for an article to be found\n",
    "    OUTPUT:  articles:           DATAFRAME, contains all articles from GDELT-API matching the input parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # define base url for GDELT-API\n",
    "    base='https://api.gdeltproject.org/api/v2/doc/doc?query='\n",
    "    \n",
    "    # convert dates\n",
    "    start_dt=datetime.datetime.strptime(start,\"%y%m%d\")\n",
    "    end_dt=datetime.datetime.strptime(end,\"%y%m%d\")\n",
    "    iter_dt=start_dt\n",
    "    \n",
    "    # initialize articles-dataframe\n",
    "    cols=['url','title','seendate','domain']\n",
    "    articles=pd.DataFrame(columns=cols)\n",
    "    \n",
    "     # iterate in steps of 1 day\n",
    "    while iter_dt+timedelta(days=1)<=end_dt:\n",
    "        \n",
    "        # define start and end time for current search\n",
    "        cur_start=str(iter_dt).split(' ')[0].replace('-','')\n",
    "        cur_end=str(iter_dt+timedelta(days=1)).split(' ')[0].replace('-','')\n",
    "        \n",
    "        # generate url\n",
    "        if phrase2==0:\n",
    "            url=base+'domainis:'+domain+' \"'+phrase+'\"'+'&sourcelang:english&format=json&STARTDATETIME='+cur_start+'000000&ENDDATETIME='+cur_end+'000000&MAXRECORDS=250'\n",
    "        else:\n",
    "            url=base+'domainis:'+domain+' (\"'+phrase+'\"'+' OR '+' \"'+phrase2+'\")'+'&sourcelang:english&format=json&STARTDATETIME='+cur_start+'000000&ENDDATETIME='+cur_end+'000000&MAXRECORDS=250'\n",
    "        \n",
    "        # start time\n",
    "        starttime=time.time()\n",
    "        \n",
    "        # visit url, read json, append results to articles\n",
    "        r=requests.get(url)\n",
    "        j=r.text.lstrip('{\"articles\": ').rstrip(' }')\n",
    "        if j!='':\n",
    "            results=pd.read_json(StringIO(j))\n",
    "            articles=articles.append(results[cols]).reset_index(drop=True)\n",
    "            print(cur_start+': '+str(len(results))+' articles found. Total articles: '+str(len(articles))+'.')\n",
    "        else:\n",
    "            print(cur_start+': No new articles found.')\n",
    "        iter_dt=iter_dt+timedelta(days=1)\n",
    "        \n",
    "        # check elapsed time, if less than 5 seconds: implement a delay\n",
    "        elapsedtime=time.time()-starttime\n",
    "        if elapsedtime<5:\n",
    "            time.sleep(5-elapsedtime)\n",
    "    \n",
    "    # drop duplicate articles\n",
    "    n_before=len(articles)\n",
    "    articles=articles.drop_duplicates(subset=['url'])\n",
    "    print('Deleted '+str(n_before-len(articles))+' articles. '+str(len(articles))+' remaining.')\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_phrases(domain, start, end, selected_counts_path, gdelt_dir, joint_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Finds all articles of a given outlet in a given time frame containing one or two phrases for a loop of multiple n-grams.\n",
    "    INPUTS:  domain:                STRING, domain\n",
    "             start:                 STRING, start date in yymmdd-format\n",
    "             end:                   STRING, end date in yymmdd-format\n",
    "             selected_counts_path:  STRING, path where selected n-grams of a certain experiment are defined\n",
    "             gdelt_dir:             STRING, directory where GDELT-results are saved\n",
    "             joint_dir:             STRING, directory of event analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # select domain name\n",
    "    domain_stripped=domain.split('.')[0]\n",
    "    \n",
    "    # import selected n-grams and the accompanying phrases\n",
    "    selected_counts=pd.read_csv(selected_counts_path)\n",
    "    selected_counts=selected_counts.fillna(0)\n",
    "    \n",
    "    # generate non-existent folder\n",
    "    folder=gdelt_dir+start+'_'+end+'/'+domain_stripped+'/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # generate list of n-grams for which articles of given domain have already been searched\n",
    "    done=[]\n",
    "    for index, filename in enumerate(os.listdir(folder)):\n",
    "        done.append(filename.split('.')[0].replace('_',' '))\n",
    "        \n",
    "    # iterate through n-grams\n",
    "    for i in range(len(selected_counts)):\n",
    "        print(i)\n",
    "        selected_ngram=selected_counts.loc[i,'ngrams']\n",
    "        \n",
    "        # if articles already searched, import results and save length\n",
    "        if selected_ngram.translate(str.maketrans('', '', string.punctuation)) in done:\n",
    "            print('Ngram already searched for '+domain+'.')\n",
    "            articles=pd.read_csv(folder+selected_ngram.translate(str.maketrans('', '', string.punctuation)).replace(' ','_')+'.csv')\n",
    "            selected_counts.loc[i,'articles']=len(articles)\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # set phrase and phrase2\n",
    "            phrase=selected_counts.loc[i,'phrase_freq_1']\n",
    "            phrase2=selected_counts.loc[i,'phrase_freq_2']\n",
    "            \n",
    "            # call single_phrase\n",
    "            articles=single_phrase(domain, phrase, start, end, phrase2)\n",
    "            articles['ngram']=selected_ngram\n",
    "            ngram=selected_ngram.translate(str.maketrans('', '', string.punctuation)).replace(' ','_')\n",
    "            \n",
    "            # save results\n",
    "            articles.to_csv(folder+ngram+'.csv',index=False)\n",
    "            selected_counts.loc[i,'articles']=len(articles)\n",
    "    \n",
    "    # generate non-existent folder\n",
    "    new_selected_counts_folder=joint_dir+'selected_counts_by_domain/'\n",
    "    if not os.path.exists(new_selected_counts_folder): \n",
    "        os.makedirs(new_selected_counts_folder)\n",
    "    \n",
    "    # save results\n",
    "    selected_counts.to_csv(new_selected_counts_folder+domain_stripped+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Measuring Slant (Gentzkow & Shapiro, 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs2010_reg1(joint_dir, n, topic0, topic1):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates regression data and coefficients for the approach of Gentzkow & Shapiro (2010).\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "             topic0:       STRING, name of topic relating to extreme 0\n",
    "             topic1:       STRING, name of topic relating to extreme 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # import or generate selected_counts-file\n",
    "    if os.path.exists(joint_dir+'selected_counts_'+str(n)+'grams.csv'):\n",
    "        selected_counts = pd.read_csv(joint_dir+'selected_counts_'+str(n)+'grams.csv')\n",
    "    else:\n",
    "        sc2 = pd.read_csv(joint_dir+'selected_counts_2grams.csv')\n",
    "        sc3 = pd.read_csv(joint_dir+'selected_counts_3grams.csv')\n",
    "        selected_counts = sc2.append(sc3).reset_index(drop=True)\n",
    "    \n",
    "    # generate list of n-grams\n",
    "    ngrams = []\n",
    "    for i in range(len(selected_counts)):\n",
    "        ngrams.append(selected_counts.loc[i,'ngrams'])\n",
    "    \n",
    "    # combine stemmed tweets of both extremes\n",
    "    extreme1=pd.read_csv(joint_dir+topic1+'_stemmed.csv')\n",
    "    extreme0=pd.read_csv(joint_dir+topic0+'_stemmed.csv')\n",
    "    extreme1['ideology']=1\n",
    "    extreme0['ideology']=0\n",
    "    tweets=extreme0.append(extreme1).dropna().reset_index(drop=True)\n",
    "    \n",
    "    # generate list of accounts\n",
    "    accounts=list(dict.fromkeys(list(tweets['acc'])))\n",
    "    \n",
    "    # if only bigrams or trigrams\n",
    "    if n!='2_3':\n",
    "        \n",
    "        # regression data\n",
    "        reg_data=pd.DataFrame()\n",
    "        reg_data['acc']=accounts\n",
    "        \n",
    "        for acc in range(len(reg_data)):\n",
    "            \n",
    "            # set ideology\n",
    "            selected_tweets=tweets.loc[tweets['acc']==reg_data.loc[acc,'acc']].dropna().reset_index(drop=True)\n",
    "            reg_data.loc[acc,'ideology']=selected_tweets.loc[0,'ideology']\n",
    "            reg_data.loc[acc,'tweets']=len(selected_tweets)\n",
    "            \n",
    "            # compute frequencies\n",
    "            for tweet in range(len(selected_tweets)):\n",
    "                if n==2:\n",
    "                    selected_tweets.loc[tweet,'ngrams']=str(list(nltk.bigrams(nltk.word_tokenize(selected_tweets.loc[tweet,'tweet']))))\n",
    "                if n==3:\n",
    "                    selected_tweets.loc[tweet,'ngrams']=str(list(nltk.trigrams(nltk.word_tokenize(selected_tweets.loc[tweet,'tweet']))))\n",
    "            for nn in range(len(ngrams)):\n",
    "                ngram = ngrams[nn]\n",
    "                occ=len(selected_tweets.loc[selected_tweets['ngrams'].str.contains(ngram)])\n",
    "                reg_data.loc[acc,'freq'+str(nn)]=occ/len(selected_tweets)\n",
    "        \n",
    "        # save\n",
    "        reg_data.to_csv(joint_dir+'gs2010_reg1_data_'+str(n)+'grams.csv',index=False)\n",
    "    \n",
    "    # if combination of bigrams and trigrams\n",
    "    else:\n",
    "        \n",
    "        # import data and merge\n",
    "        data0 = pd.read_csv(joint_dir+'gs2010_reg1_data_2grams.csv')\n",
    "        data1 = pd.read_csv(joint_dir+'gs2010_reg1_data_3grams.csv')\n",
    "        reg_data = pd.merge(data0, data1, on=['acc', 'ideology', 'tweets'], how='outer')\n",
    "        names = ['acc', 'ideology', 'tweets']\n",
    "        for i in range(len(data0.columns)+len(data1.columns)-6):\n",
    "            names.append('freq'+str(i))\n",
    "        reg_data.columns = names\n",
    "        \n",
    "        # save\n",
    "        reg_data.to_csv(joint_dir+'gs2010_reg1_data_'+str(n)+'grams.csv')\n",
    "    \n",
    "    # perform regressions\n",
    "    coefficients = pd.DataFrame(columns=['ngram','a','b'])\n",
    "    coefficients['ngram'] = ngrams\n",
    "    for i in range(len(coefficients)):\n",
    "        x = pd.DataFrame(reg_data['ideology'])\n",
    "        x = sm.add_constant(x)\n",
    "        y = pd.DataFrame(reg_data['freq'+str(i)])\n",
    "        model = sm.OLS(y.astype(float), x.astype(float)).fit()\n",
    "        coefficients.loc[i,'a'] = model.params[0]\n",
    "        coefficients.loc[i,'b'] = model.params[1]\n",
    "    \n",
    "    # save regression results\n",
    "    coefficients.to_csv(joint_dir+'gs2010_reg1_coefficients_'+str(n)+'grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs2010_est_twitter_ideologies(joint_dir, n):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimates ideologies of the Twitter accounts in the bubbles with the approach of Gentzkow & Shapiro (2010).\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import regression data and coefficients\n",
    "    reg_data = pd.read_csv(joint_dir+'gs2010_reg1_data_'+str(n)+'grams.csv')\n",
    "    coefficients = pd.read_csv(joint_dir+'gs2010_reg1_coefficients_'+str(n)+'grams.csv')\n",
    "    \n",
    "    # generate dataframe showing true account ideologies\n",
    "    ai_cols = ['acc', 'ideology']\n",
    "    acc_ideologies = reg_data[ai_cols]\n",
    "    \n",
    "    # calculate estimated ideology for each account; as well as round estimated value to nearest extreme; compute correct percentage\n",
    "    for acc in range(len(acc_ideologies)):\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        for i in range(0,len(coefficients)):\n",
    "            num = num + coefficients.loc[i, 'b'] * (reg_data.loc[acc,'freq'+str(i)] - coefficients.loc[i, 'a'])\n",
    "            denom = denom + (coefficients.loc[i, 'b'])**2\n",
    "            acc_ideologies.loc[acc,'est_ideology'] = num / denom\n",
    "        \n",
    "        # rounded_est\n",
    "        if acc_ideologies.loc[acc,'est_ideology'] < 0.5:\n",
    "            acc_ideologies.loc[acc,'rounded_est'] = 0\n",
    "        else:\n",
    "            acc_ideologies.loc[acc,'rounded_est'] = 1\n",
    "        \n",
    "        # correct\n",
    "        if acc_ideologies.loc[acc,'rounded_est'] == acc_ideologies.loc[acc,'ideology']:\n",
    "            acc_ideologies.loc[acc, 'correct'] = 1\n",
    "        else:\n",
    "            acc_ideologies.loc[acc, 'correct'] = 0\n",
    "    \n",
    "    # calculate correlation between true and estimated ideologies\n",
    "    corr = np.corrcoef(acc_ideologies['ideology'], acc_ideologies['est_ideology'])[0][1]\n",
    "    \n",
    "    print('Correlation true/estimated ideology: ' + str(round(corr,4)))\n",
    "    print('Rounded estimate percentage correct: ' + str(round(acc_ideologies['correct'].mean()*100,4)) + '%')\n",
    "    print('Variation in slant attributable to variation in ideology: ' + str(round((corr**2)*100,4)) + '%')\n",
    "    print('Variation in slant attributable to noise: ' + str(round((1-corr**2)*100,4)) + '%')\n",
    "    \n",
    "    # save results\n",
    "    acc_ideologies.to_csv(joint_dir+'gs2010_est_acc_ideologies_'+str(n)+'grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs2010_ideology_estimate(domain, start, end, joint_dir, gdelt_dir, n):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimates ideology of an outlet with the approach of Gentzkow & Shapiro (2010).\n",
    "    INPUTS:  domain:       STRING, domain\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # define time and domain name\n",
    "    time = start + '_' + end\n",
    "    domain_stripped = domain.split('.')[0]\n",
    "    \n",
    "    # import coefficient data\n",
    "    gentzkow_coef = pd.read_csv(joint_dir+'/gs2010_reg1_coefficients_'+str(n)+'grams.csv')\n",
    "    \n",
    "    # compute number of all articles of selected domain in selected timeframe\n",
    "    all_articles = pd.read_csv(gdelt_dir + 'domains/' + domain_stripped + '_' + time + '.csv')\n",
    "    all_articles = all_articles.loc[all_articles['domain']==domain].reset_index(drop=True)\n",
    "    total_articles = len(all_articles)\n",
    "    \n",
    "    # compute ideology\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for i in range(len(gentzkow_coef)):\n",
    "        ngram_cleaned = gentzkow_coef.loc[i, 'ngram'].translate(str.maketrans('', '', string.punctuation)).replace(' ','_')\n",
    "        articles = pd.read_csv(gdelt_dir + time + '/' + domain_stripped + '/' + ngram_cleaned + '.csv')\n",
    "        articles = articles.loc[articles['domain']==domain].reset_index(drop=True)\n",
    "        selected_articles = len(articles)\n",
    "        gentzkow_coef.loc[i, 'freq'] = selected_articles / total_articles\n",
    "        spec_num = gentzkow_coef.loc[i, 'b'] * (gentzkow_coef.loc[i,'freq'] - gentzkow_coef.loc[i, 'a'])\n",
    "        gentzkow_coef.loc[i, 'num'] = spec_num\n",
    "        gentzkow_coef.loc[i, 'absnum'] = abs(spec_num)\n",
    "        num = num + spec_num\n",
    "        denom = denom + (gentzkow_coef.loc[i, 'b'])**2\n",
    "    if not os.path.exists(joint_dir+'outlets/'): \n",
    "        os.makedirs(joint_dir+'outlets/')\n",
    "    gentzkow_coef = gentzkow_coef.sort_values(by='absnum',ascending=False)\n",
    "    gentzkow_coef.to_excel(joint_dir+'outlets/'+domain+'.xlsx',index=False)\n",
    "    est_ideology = num / denom\n",
    "    print('Estimated ideology ' + domain + ': ' + str(round(est_ideology,2)))\n",
    "    \n",
    "    # save results\n",
    "    if not os.path.exists(joint_dir+'/gs2010_est_outlet_ideologies_'+str(n)+'grams.csv'): \n",
    "        save = pd.DataFrame()\n",
    "        save.loc[0,'domain'] = domain\n",
    "        save.loc[0,'ideology'] = est_ideology\n",
    "        save.to_csv(joint_dir+'/gs2010_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)\n",
    "    else:\n",
    "        save = pd.read_csv(joint_dir+'/gs2010_est_outlet_ideologies_'+str(n)+'grams.csv')\n",
    "        l = len(save)\n",
    "        save.loc[l,'domain'] = domain\n",
    "        save.loc[l,'ideology'] = est_ideology\n",
    "        save = save.drop_duplicates(subset='domain', keep='last')\n",
    "        save.to_csv(joint_dir+'/gs2010_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs2010_ideology_estimates(domains, start, end, joint_dir, gdelt_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates ideologies of outlets with the approach of Gentzkow & Shapiro (2010).\n",
    "    INPUTS:  domains:      LIST, domains\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # loop over domains\n",
    "    for i in range(len(domains)):\n",
    "        gs2010_ideology_estimate(domains[i], start, end, joint_dir, gdelt_dir, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Adjusting Outlet Phrase Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs2010_ideology_estimates_adj(domains, start, end, joint_dir, gdelt_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates ideologies of outlets with the adjusted approach of Gentzkow & Shapiro (2010).\n",
    "    INPUTS:  domains:      LIST, domains\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(domains)):\n",
    "        domain = domains[i]\n",
    "        \n",
    "        # define time and domain name\n",
    "        time = start + '_' + end\n",
    "        domain_stripped = domain.split('.')[0]\n",
    "        \n",
    "        # import coefficient data\n",
    "        gentzkow_coef = pd.read_csv(joint_dir+'/gs2010_reg1_coefficients_'+str(n)+'grams.csv')\n",
    "        \n",
    "        # compute number of all articles of selected domain in selected timeframe\n",
    "        all_articles = pd.read_csv(gdelt_dir + 'domains/' + domain_stripped + '_' + time + '.csv')\n",
    "        all_articles = all_articles.loc[all_articles['domain']==domain].reset_index(drop=True)\n",
    "        total_articles = len(all_articles)\n",
    "        \n",
    "        # compute ideology\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        for i in range(len(gentzkow_coef)):\n",
    "            ngram_cleaned = gentzkow_coef.loc[i, 'ngram'].translate(str.maketrans('', '', string.punctuation)).replace(' ','_')\n",
    "            articles = pd.read_csv(gdelt_dir + time + '/' + domain_stripped + '/' + ngram_cleaned + '.csv')\n",
    "            articles = articles.loc[articles['domain']==domain].reset_index(drop=True)\n",
    "            selected_articles = len(articles)\n",
    "            gentzkow_coef.loc[i, 'freq'] = selected_articles / total_articles * 0.028\n",
    "            num = num + gentzkow_coef.loc[i, 'b'] * (gentzkow_coef.loc[i,'freq'] - gentzkow_coef.loc[i, 'a'])\n",
    "            denom = denom + (gentzkow_coef.loc[i, 'b'])**2\n",
    "        if not os.path.exists(joint_dir+'outlets/'): \n",
    "            os.makedirs(joint_dir+'outlets/')\n",
    "        gentzkow_coef.to_excel(joint_dir+'outlets/'+domain+'.xlsx',index=False)\n",
    "        est_ideology = num / denom\n",
    "        print('Estimated ideology ' + domain + ': ' + str(round(est_ideology,2)))\n",
    "        \n",
    "        # save results\n",
    "        if not os.path.exists(joint_dir+'/gs2010_adj_est_outlet_ideologies_'+str(n)+'grams.csv'): \n",
    "            save = pd.DataFrame()\n",
    "            save.loc[0,'domain'] = domain\n",
    "            save.loc[0,'ideology'] = est_ideology\n",
    "            save.to_csv(joint_dir+'/gs2010_adj_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)\n",
    "        else:\n",
    "            save = pd.read_csv(joint_dir+'/gs2010_adj_est_outlet_ideologies_'+str(n)+'grams.csv')\n",
    "            l = len(save)\n",
    "            save.loc[l,'domain'] = domain\n",
    "            save.loc[l,'ideology'] = est_ideology\n",
    "            save = save.drop_duplicates(subset='domain', keep='last')\n",
    "            save.to_csv(joint_dir+'/gs2010_adj_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Jensen et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen2012_betas(joint_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates coefficients for the approach of Jensen et al. (2012).\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import data, change ideology 0 to -1\n",
    "    data = pd.read_csv(joint_dir+'gs2010_reg1_data_'+str(n)+'grams.csv')\n",
    "    data['ideology'] = data['ideology'].replace(0.0,-1.0)\n",
    "    \n",
    "    # setup betas-dataframe\n",
    "    betas = pd.DataFrame(columns=['beta'])\n",
    "    \n",
    "    # calculate beta coefficients\n",
    "    for i in range(len(data.columns)-3):\n",
    "        ngram = 'freq'+str(i)\n",
    "        beta = 0\n",
    "        for acc in range(len(data)):\n",
    "            beta = beta + data.loc[acc, ngram]*data.loc[acc,'ideology']\n",
    "        betas.loc[i,'beta'] = beta\n",
    "    \n",
    "    # save results\n",
    "    betas.to_csv(joint_dir+'jensen2012_betas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen2012_est_twitter_ideologies(joint_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates ideologies of the Twitter accounts in the bubbles with the approach of Jensen et al. (2012).\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import regression data and coefficients\n",
    "    data = pd.read_csv(joint_dir+'gs2010_reg1_data_'+str(n)+'grams.csv')\n",
    "    data['ideology'] = data['ideology'].replace(0.0,-1.0)\n",
    "    betas = pd.read_csv(joint_dir+'jensen2012_betas.csv')\n",
    "    \n",
    "    # generate dataframe showing true account ideologies\n",
    "    ai_cols = ['acc', 'ideology']\n",
    "    acc_ideologies = data[ai_cols]\n",
    "    \n",
    "    # calculate estimated ideology for each account; as well as round estimated value to nearest extreme; compute correct percentage\n",
    "    for acc in range(len(acc_ideologies)):\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        for i in range(0,len(betas)):\n",
    "            num = num + (betas.loc[i, 'beta'] * (data.loc[acc,'freq'+str(i)]))\n",
    "            denom = denom + data.loc[acc,'freq'+str(i)]\n",
    "            acc_ideologies.loc[acc,'est_ideology'] = num / denom\n",
    "        # if no n-gram used by account\n",
    "        if pd.isnull(acc_ideologies.loc[acc,'est_ideology']):\n",
    "            acc_ideologies.loc[acc,'est_ideology'] = 0\n",
    "        # rounded_est\n",
    "        if acc_ideologies.loc[acc,'est_ideology'] < 0:\n",
    "            acc_ideologies.loc[acc,'rounded_est'] = -1\n",
    "        else:\n",
    "            acc_ideologies.loc[acc,'rounded_est'] = 1\n",
    "        # correct\n",
    "        if acc_ideologies.loc[acc,'rounded_est'] == acc_ideologies.loc[acc,'ideology']:\n",
    "            acc_ideologies.loc[acc, 'correct'] = 1\n",
    "        else:\n",
    "            acc_ideologies.loc[acc, 'correct'] = 0\n",
    "    \n",
    "    # calculate correlation between true and estimated ideologies\n",
    "    corr = np.corrcoef(acc_ideologies['ideology'], acc_ideologies['est_ideology'])[0][1]\n",
    "    \n",
    "    print('Correlation true/estimated ideology: ' + str(round(corr,2)))\n",
    "    print('Rounded estimate percentage correct: ' + str(round(acc_ideologies['correct'].mean()*100,2)) + '%')\n",
    "    print('Variation in slant attributable to variation in ideology: ' + str(round((corr**2)*100,0)) + '%')\n",
    "    print('Variation in slant attributable to noise: ' + str(round((1-corr**2)*100,0)) + '%')\n",
    "    \n",
    "    # save results\n",
    "    acc_ideologies.to_csv(joint_dir+'jensen2012_est_acc_ideologies_'+str(n)+'grams.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen2012_ideology_estimate(domains, start, end, joint_dir, gdelt_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates ideologies of outlets with the adjusted approach of Jensen et al. (2012).\n",
    "    INPUTS:  domains:      LIST, domains\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams OR STRING: '2_3' for combination of bigrams and trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    for domain in range(len(domains)):\n",
    "        domain = domains[domain]\n",
    "        \n",
    "        # define time and domain rame\n",
    "        time = start + '_' + end\n",
    "        domain_stripped = domain.split('.')[0]\n",
    "        \n",
    "        # import coefficient data\n",
    "        betas = pd.read_csv(joint_dir+'jensen2012_betas.csv')\n",
    "        sc = pd.read_csv(joint_dir+'selected_counts_'+str(n)+'grams.csv')\n",
    "        betas['ngram'] = sc['ngrams']\n",
    "        \n",
    "        # compute number of all articles of selected domain in selected timeframe\n",
    "        all_articles = pd.read_csv(gdelt_dir + 'domains/' + domain_stripped + '_' + time + '.csv')\n",
    "        all_articles = all_articles.loc[all_articles['domain']==domain].reset_index(drop=True)\n",
    "        total_articles = len(all_articles)\n",
    "        \n",
    "        # compute ideology\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        for i in range(len(betas)):\n",
    "            ngram_cleaned = betas.loc[i, 'ngram'].translate(str.maketrans('', '', string.punctuation)).replace(' ','_')\n",
    "            articles = pd.read_csv(gdelt_dir + time + '/' + domain_stripped + '/' + ngram_cleaned + '.csv')\n",
    "            articles = articles.loc[articles['domain']==domain].reset_index(drop=True)\n",
    "            selected_articles = len(articles)\n",
    "            betas.loc[i, 'freq'] = selected_articles / total_articles\n",
    "            num = num + (betas.loc[i,'freq']*betas.loc[i,'beta'])\n",
    "            denom = denom + betas.loc[i,'freq']\n",
    "        est_ideology = num / denom\n",
    "        print('Estimated ideology ' + domain + ': ' + str(round(est_ideology,2)))\n",
    "        \n",
    "        # save results\n",
    "        if not os.path.exists(joint_dir+'/jensen2012_est_outlet_ideologies_'+str(n)+'grams.csv'): \n",
    "            save = pd.DataFrame()\n",
    "            save.loc[0,'domain'] = domain\n",
    "            save.loc[0,'ideology'] = est_ideology\n",
    "            save.to_csv(joint_dir+'/jensen2012_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)\n",
    "        else:\n",
    "            save = pd.read_csv(joint_dir+'/jensen2012_est_outlet_ideologies_'+str(n)+'grams.csv')\n",
    "            l = len(save)\n",
    "            save.loc[l,'domain'] = domain\n",
    "            save.loc[l,'ideology'] = est_ideology\n",
    "            save = save.drop_duplicates(subset='domain', keep='last')\n",
    "            save.to_csv(joint_dir+'/jensen2012_est_outlet_ideologies_'+str(n)+'grams.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Finding Omitted Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(full_domain, gdelt_dir, start, end):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape articles of given domains.\n",
    "    INPUTS:  full_domain:  STRING, domain\n",
    "             gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "    \"\"\"\n",
    "    \n",
    "    # define domain name\n",
    "    domain = full_domain.split('.')[0]\n",
    "    \n",
    "    # import articles found on GDELT-API for given domain\n",
    "    articles = pd.read_csv(gdelt_dir+'domains/'+domain+'_'+start+'_'+end+'.csv')\n",
    "    articles = articles.loc[articles['domain']==full_domain].reset_index(drop=True)\n",
    "    \n",
    "    # foxnews\n",
    "    if domain == 'foxnews':\n",
    "        \n",
    "        # iterate through articles\n",
    "        for i in range(len(articles)):\n",
    "            print(i)\n",
    "            url = articles.loc[i,'url']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            \n",
    "            # find all p-elements in div 'article-body'\n",
    "            elements = soup.find_all('div', {\"class\": ['article-body']})\n",
    "            t = 0\n",
    "            for e in range(len(elements)):\n",
    "                text = elements[e].find_all('p')\n",
    "                if len(text)>0:\n",
    "                    for f in range(len(text)):\n",
    "                        text_p = text[f].text\n",
    "                        articles.loc[i,'text'+str(t)] = text_p\n",
    "                        t = t + 1\n",
    "            \n",
    "            # find all div 'description', used for picture galleries\n",
    "            if len(elements)==0:\n",
    "                elements = soup.find_all('div', {\"itemprop\": ['description']})\n",
    "                for f in range(len(elements)):\n",
    "                    text_p = elements[f].text.replace('\\n', ' ')\n",
    "                    articles.loc[i,'text'+str(t)] = text_p\n",
    "                    print(text_p)\n",
    "                    t = t + 1\n",
    "    \n",
    "    # cnn\n",
    "    if domain == 'cnn':\n",
    "        articles = articles.loc[~articles['url'].str.contains('cnn-underscored')].reset_index(drop=True)\n",
    "        for i in range(len(articles)):\n",
    "            print(i)\n",
    "            url = articles.loc[i,'url']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            \n",
    "            # find all div-elements with selected classes\n",
    "            elements = soup.find_all('div', {\"class\": ['cn__column cn__column--2up0', \n",
    "                        'el__leafmedia el__leafmedia--sourced-paragraph', \n",
    "                        'zn-body__paragraph', \n",
    "                        'Paragraph__component BasicArticle__paragraph BasicArticle__pad Paragraph__isDropCap',\n",
    "                        'Paragraph__component BasicArticle__paragraph BasicArticle__pad',\n",
    "                        'sc-bdVaJa post-content-rendered render-stellar-contentstyles__Content-sc-9v7nwy-0 erzhuK',\n",
    "                        'Paragraph__component',\n",
    "                        'article__content',\n",
    "                        'sc-bdVaJa post-content-rendered render-stellar-contentstyles__Content-sc-9v7nwy-0 dUdYqp',\n",
    "                        'cnnix__article__intro',\n",
    "                        'SpecialArticle__headDescription',\n",
    "                        'SpecialArticle__paragraph SpecialArticle__pad SpecialArticle__widthStandard SpecialArticle__isDropCap',\n",
    "                        'SpecialArticle__paragraph SpecialArticle__pad SpecialArticle__widthStandard',\n",
    "                        'sc-bdVaJa post-content-rendered render-stellar-contentstyles__Content-sc-9v7nwy-0 dUdYqp',\n",
    "                        'sc-dnqmqq render-stellar-contentstyles__List-sc-9v7nwy-1 eUPcFX',\n",
    "                        'sc-bdVaJa post-content-rendered render-stellar-contentstyles__Content-sc-9v7nwy-0 daEDKg',\n",
    "                        'pg-special-article__head pg-special-article__head-',\n",
    "                        'el-unfurled__caption',\n",
    "                        'block-grid']})\n",
    "            t = 0\n",
    "            for e in range(len(elements)):\n",
    "                text = elements[e].text.replace('\\n', ' ')\n",
    "                articles.loc[i,'text'+str(t)] = text\n",
    "                t = t + 1\n",
    "    \n",
    "    # alternet\n",
    "    if domain == 'alternet':\n",
    "        for i in range(len(articles)):\n",
    "            url = articles.loc[i,'url']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # find all p-elements in div 'article-body'\n",
    "            elements = soup.find_all('div', {\"class\": ['body-description']})\n",
    "            t = 0\n",
    "            for e in range(len(elements)):\n",
    "                text = elements[e].find_all('p')\n",
    "                if len(text)>0:\n",
    "                    for f in range(len(text)):\n",
    "                        text_p = text[f].text\n",
    "                        articles.loc[i,'text'+str(t)] = text_p\n",
    "                        t = t + 1\n",
    "    # usatoday\n",
    "    if domain == 'usatoday':\n",
    "        for i in range(len(articles)):\n",
    "            try:\n",
    "                url = articles.loc[i,'url']\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "                # find all p-elements in div 'article-body'\n",
    "                elements = soup.find_all('div', {\"id\": ['truncationWrap']})\n",
    "                t = 0\n",
    "                for e in range(len(elements)):\n",
    "                    text = elements[e].find_all('p')\n",
    "                    if len(text)>0:\n",
    "                        for f in range(len(text)):\n",
    "                            text_p = text[f].text\n",
    "                            articles.loc[i,'text'+str(t)] = text_p\n",
    "                            t = t + 1\n",
    "                \n",
    "                # for pictures/videos\n",
    "                if len(elements)==0:\n",
    "                    elements = soup.find_all('div', {\"class\": ['detail-text']})\n",
    "                    for f in range(len(elements)):\n",
    "                        text_p = elements[f].text.replace('\\n', ' ')\n",
    "                        articles.loc[i,'text'+str(t)] = text_p\n",
    "                        t = t + 1\n",
    "                \n",
    "                # more div classes\n",
    "                if len(elements)==0:\n",
    "                    elements = soup.find_all('div', {\"class\": ['article-wrapper', 'article-inner theme-light', 'article-inner theme-dark']})\n",
    "                    for e in range(len(elements)):\n",
    "                        text = elements[e].find_all('p')\n",
    "                        if len(text)>0:\n",
    "                            for f in range(len(text)):\n",
    "                                text_p = text[f].text\n",
    "                                articles.loc[i,'text'+str(t)] = text_p\n",
    "                                t = t + 1   \n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # breitbart\n",
    "    if domain == 'breitbart':\n",
    "        for i in range(len(articles)):\n",
    "            url = articles.loc[i,'url']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # find all p-elements in div 'article-body'\n",
    "            elements = soup.find_all('div', {\"class\": ['entry-content']})\n",
    "            t = 0\n",
    "            for e in range(len(elements)):\n",
    "                text = elements[e].find_all('p')\n",
    "                if len(text)>0:\n",
    "                    for f in range(len(text)):\n",
    "                        text_p = text[f].text\n",
    "                        articles.loc[i,'text'+str(t)] = text_p\n",
    "                        t = t + 1        \n",
    "    \n",
    "    # save results\n",
    "    articles.to_csv(gdelt_dir+'fulltexts/'+domain+'_'+start+'_'+end+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_omit(gdelt_dir, domain, start, end):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares scraped article data.\n",
    "    INPUTS:  gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             domain:       STRING, domain\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "    \"\"\"\n",
    "    \n",
    "    # read article data\n",
    "    articles = pd.read_csv(gdelt_dir+'fulltexts/'+domain.split('.')[0]+'_'+start+'_'+end+'.csv')\n",
    "    del articles['url']\n",
    "    del articles['title']\n",
    "    del articles['seendate']\n",
    "    del articles['domain']\n",
    "    \n",
    "    # collect text snippets\n",
    "    text = []\n",
    "    for row in range(len(articles)):\n",
    "        for i in range(len(articles.columns)):\n",
    "            if not pd.isnull(articles.loc[row, 'text'+str(i)]):\n",
    "                text.append(articles.loc[row, 'text'+str(i)].replace(u'\\xa0', u' ').replace(u'\\\\', u' '))\n",
    "    \n",
    "    # save unique text snippets\n",
    "    text_unique = list(dict.fromkeys(text))\n",
    "    pd.DataFrame(text_unique, columns=['text']).to_csv(gdelt_dir+'fulltexts/'+domain.split('.')[0]+'_unique'+'_'+start+'_'+end+'.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_articles(gdelt_dir, joint_dir, domain, start, end, a, b, c):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans and stems scraped article data, and removes stop words.\n",
    "    INPUTS:  gdelt_dir:    STRING, directory where GDELT-results are saved\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             domain:       STRING, domain\n",
    "             start:        STRING, start date in yymmdd-format\n",
    "             end:          STRING, end date in yymmdd-format\n",
    "             a:            STRING, special character '’'\n",
    "             b:            STRING, special character '”'\n",
    "             c:            STRING, special character '“'             \n",
    "    \"\"\"\n",
    "    \n",
    "    # import full text\n",
    "    text = pd.read_csv(gdelt_dir+'fulltexts/'+domain.split('.')[0]+'_unique'+'_'+start+'_'+end+'.csv')\n",
    "    text_save = text.copy()\n",
    "    \n",
    "    # initialize Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    own_stopwords = ['cannot', 'gonna', 'gotta', 'im', 'ive', 'like', 'cant', 'whats', 'wanna', 'us', 'amp', 'lets', 'gimme', 'gimmee']\n",
    "    \n",
    "    # generate list of stop words\n",
    "    stop_np = stop_words(own_stopwords, a)\n",
    "    \n",
    "    for t in range(len(text)):\n",
    "        \n",
    "        # split, remove emojis, replace special characters\n",
    "        split = text.loc[t,'text'].encode('ascii', 'ignore').decode('ascii').replace(a,\"\").replace(b,\"\").replace(c,\"\").split()\n",
    "        \n",
    "        # generate new cleaned and stemmed article text\n",
    "        if len(split)>0:\n",
    "            new_art = ''\n",
    "            new_art_cl = ''\n",
    "            for element in range(len(split)):\n",
    "                if split[element] in stop_np:\n",
    "                    new_art_cl = new_art_cl + split[element].translate(str.maketrans('', '', string.punctuation)).lower() + ' '\n",
    "                    continue\n",
    "                else:\n",
    "                    new_art = new_art + stemmer.stem(split[element].translate(str.maketrans('', '', string.punctuation)).lower()) + ' '\n",
    "                    new_art_cl = new_art_cl + split[element].translate(str.maketrans('', '', string.punctuation)).lower() + ' '\n",
    "            text.loc[t,'text'] = new_art\n",
    "            text_save.loc[t,'text'] = new_art_cl\n",
    "        else:\n",
    "            text.loc[t,'text'] = np.nan\n",
    "            text_save.loc[t,'text'] = np.nan\n",
    "    \n",
    "    # generate non-existent folder\n",
    "    if not os.path.exists(joint_dir+'omit/'): \n",
    "            os.makedirs(joint_dir+'omit/')\n",
    "    \n",
    "    # save stemmed and cleaned text\n",
    "    text.to_csv(joint_dir+'omit/'+domain.split('.')[0]+'_stemmed.csv', index=False)\n",
    "    text_save.to_csv(joint_dir+'omit/'+domain.split('.')[0]+'_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extreme(domains, joint_dir, extreme):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans and stems scraped article data, and removes stop words.\n",
    "    INPUTS:  domains:      LIST, list of domains belonging to one extreme\n",
    "             joint_dir:    STRING, directory of event analysis\n",
    "             extreme:      INTEGER, 0 or 1          \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize empty dataframes\n",
    "    s = pd.DataFrame(columns=['text'])\n",
    "    c = pd.DataFrame(columns=['text'])\n",
    "    \n",
    "    # append data from domains\n",
    "    for i in range(len(domains)):\n",
    "        stemmed = pd.read_csv(joint_dir+'omit/'+domains[i].split('.')[0]+'_stemmed.csv')\n",
    "        cleaned = pd.read_csv(joint_dir+'omit/'+domains[i].split('.')[0]+'_cleaned.csv')\n",
    "        s = s.append(stemmed)\n",
    "        c = c.append(cleaned)\n",
    "    \n",
    "    # reset indices\n",
    "    s = s.reset_index(drop=True)\n",
    "    c = c.reset_index(drop=True)\n",
    "    \n",
    "    # save resulting dataframes\n",
    "    s.to_csv(joint_dir+'omit/extreme'+str(extreme)+'_stemmed.csv', index=False)\n",
    "    c.to_csv(joint_dir+'omit/extreme'+str(extreme)+'_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phrases_omit(joint_dir, topic, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans and stems scraped article data, and removes stop words.\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             topic:        STRING, name of topic\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import data\n",
    "    data = pd.read_csv(joint_dir + 'omit/' + topic + '_cleaned.csv')\n",
    "    stem = pd.read_csv(joint_dir + 'omit/' + topic + '_stemmed.csv')\n",
    "    \n",
    "    # call function ngrams_to_phrases\n",
    "    ngrams_to_phrases(n, joint_dir + 'omit/', topic, data, stem, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_omit(joint_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the chi2-test for both extremes.\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate chi2-values\n",
    "    counts = chi_squared('extreme0', 'extreme1', joint_dir+'omit/', n)\n",
    "    \n",
    "    # import and combine phrases\n",
    "    phrases0 = pd.read_csv(joint_dir +'omit/' + 'extreme0' + '_phrases_' + str(n) + 'grams.csv')\n",
    "    phrases1 = pd.read_csv(joint_dir +'omit/' + 'extreme1' + '_phrases_' + str(n) + 'grams.csv')\n",
    "    phrases = phrases0.append(phrases1).reset_index(drop=True)\n",
    "    phrases = phrases.rename(columns={'count': 'counts'})\n",
    "    phrases = phrases.groupby(['ngram','phrase']).counts.sum().reset_index().sort_values(by='counts',ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # find phrases belonging to n-grams\n",
    "    for i in range(len(counts)):\n",
    "        ngram=counts.loc[i,'ngrams']\n",
    "        phrases_x=phrases.loc[phrases['ngram']==ngram].sort_values(by='counts',ascending=False).reset_index(drop=True)\n",
    "        if len(phrases_x.loc[0,'phrase'].replace(' ',''))>3:\n",
    "            counts.loc[i,'phrase_freq_1']=phrases_x.loc[0,'phrase']\n",
    "            if len(phrases_x)>1:\n",
    "                # second phrase has to be used more than 5 times to be considered\n",
    "                if phrases_x.loc[1,'counts']>5:\n",
    "                    if len(phrases_x.loc[1,'phrase'].replace(' ',''))>3:\n",
    "                        counts.loc[i,'phrase_freq_2']=phrases_x.loc[1,'phrase']\n",
    "    \n",
    "    # import stemmed data\n",
    "    stem0 = pd.read_csv(joint_dir +'omit/' + 'extreme0' + '_stemmed.csv')\n",
    "    stem1 = pd.read_csv(joint_dir +'omit/' + 'extreme1' + '_stemmed.csv')\n",
    "    \n",
    "    # generate most_likely-indicator\n",
    "    counts['most_likely'] = (counts['count_0']/len(stem0) < counts['count_1']/len(stem1)).astype(int)\n",
    "    \n",
    "    # save results\n",
    "    counts.to_csv(joint_dir +'omit/counts_'+str(n)+'grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omit_exclude(joint_dir, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Excludes n-grams used to measure slant.\n",
    "    INPUTS:  joint_dir:    STRING, directory of event analysis\n",
    "             n:            INTEGER, 2 for bigrams; 3 for trigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    # import n-grams sorted by chi2-values of articles\n",
    "    counts = pd.read_csv(joint_dir +'omit/counts_'+str(n)+'grams.csv')\n",
    "    \n",
    "    # import n-grams used to measure slang\n",
    "    sc = pd.read_csv(joint_dir + 'selected_counts_2grams.csv')\n",
    "    \n",
    "    # generate list of n-grams\n",
    "    ngrams = list(sc['ngrams'])\n",
    "    \n",
    "    # drop n-grams of counts that are in sc\n",
    "    if n==2:\n",
    "        for i in range(len(counts)):\n",
    "            if counts.loc[i,'ngrams'] in ngrams:\n",
    "                counts = counts.drop([i])\n",
    "    if n==3:\n",
    "        ngrams_cleaned = []\n",
    "        for i in range(len(ngrams)):\n",
    "            ngrams_cleaned.append(ngrams[i].translate(str.maketrans('', '', string.punctuation)))\n",
    "        for j in range(len(counts)):\n",
    "            split = counts.loc[j,'ngrams'].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            if (split[0] + ' ' + split[1] in ngrams_cleaned) or (split[1] + ' ' + split[2] in ngrams_cleaned):\n",
    "                counts = counts.drop([j])\n",
    "    \n",
    "    # save new counts-dataframe\n",
    "    counts.to_csv(joint_dir +'omit/counts_'+str(n)+'grams_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
